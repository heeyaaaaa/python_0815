{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 05 Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8588897\n",
      "200 0.7862256\n",
      "400 0.56168854\n",
      "600 0.4714398\n",
      "800 0.4282863\n",
      "1000 0.40248498\n",
      "1200 0.38404492\n",
      "1400 0.36919236\n",
      "1600 0.3563325\n",
      "1800 0.34472665\n",
      "2000 0.33400595\n",
      "2200 0.3239718\n",
      "2400 0.31450975\n",
      "2600 0.30554786\n",
      "2800 0.29703686\n",
      "3000 0.28893986\n",
      "3200 0.28122774\n",
      "3400 0.27387515\n",
      "3600 0.26686004\n",
      "3800 0.26016256\n",
      "4000 0.253764\n",
      "4200 0.2476474\n",
      "4400 0.24179666\n",
      "4600 0.2361968\n",
      "4800 0.23083395\n",
      "5000 0.22569476\n",
      "5200 0.2207671\n",
      "5400 0.21603924\n",
      "5600 0.21150045\n",
      "5800 0.2071405\n",
      "6000 0.20294993\n",
      "6200 0.1989197\n",
      "6400 0.1950415\n",
      "6600 0.19130743\n",
      "6800 0.18771012\n",
      "7000 0.18424268\n",
      "7200 0.18089856\n",
      "7400 0.17767154\n",
      "7600 0.17455606\n",
      "7800 0.17154664\n",
      "8000 0.16863815\n",
      "8200 0.16582586\n",
      "8400 0.16310523\n",
      "8600 0.160472\n",
      "8800 0.15792231\n",
      "9000 0.1554523\n",
      "9200 0.1530584\n",
      "9400 0.1507372\n",
      "9600 0.14848563\n",
      "9800 0.14630066\n",
      "10000 0.14417936\n",
      "\n",
      "Hypothesis:  [[0.0285363 ]\n",
      " [0.1557493 ]\n",
      " [0.29413864]\n",
      " [0.78632617]\n",
      " [0.9426478 ]\n",
      " [0.98120475]] \n",
      "Correct(Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost,train], feed_dict = {X: x_data, Y:y_data})\n",
    "        if step%200 ==0:\n",
    "            print(step, cost_val)\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect(Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('DeepLearningZeroToAll-master/data-03-diabetes.csv', delimiter = ',', dtype = np.float32)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.73231286\n",
      "200 0.5559278\n",
      "400 0.52866495\n",
      "600 0.5214396\n",
      "800 0.51752293\n",
      "1000 0.51438916\n",
      "1200 0.5115966\n",
      "1400 0.50904745\n",
      "1600 0.50670624\n",
      "1800 0.5045505\n",
      "2000 0.50256205\n",
      "2200 0.5007251\n",
      "2400 0.49902543\n",
      "2600 0.49745077\n",
      "2800 0.49598974\n",
      "3000 0.4946325\n",
      "3200 0.49337018\n",
      "3400 0.49219465\n",
      "3600 0.49109864\n",
      "3800 0.49007577\n",
      "4000 0.4891201\n",
      "4200 0.48822635\n",
      "4400 0.4873896\n",
      "4600 0.48660564\n",
      "4800 0.48587033\n",
      "5000 0.48518005\n",
      "5200 0.48453164\n",
      "5400 0.48392195\n",
      "5600 0.48334822\n",
      "5800 0.48280793\n",
      "6000 0.48229885\n",
      "6200 0.4818187\n",
      "6400 0.48136565\n",
      "6600 0.48093784\n",
      "6800 0.48053357\n",
      "7000 0.48015133\n",
      "7200 0.47978964\n",
      "7400 0.47944736\n",
      "7600 0.47912306\n",
      "7800 0.47881576\n",
      "8000 0.47852427\n",
      "8200 0.47824785\n",
      "8400 0.4779854\n",
      "8600 0.47773618\n",
      "8800 0.4774993\n",
      "9000 0.47727412\n",
      "9200 0.4770599\n",
      "9400 0.47685608\n",
      "9600 0.4766622\n",
      "9800 0.47647735\n",
      "10000 0.47630128\n",
      "\n",
      "Hypothesis:  [[0.3957755 ]\n",
      " [0.93420386]\n",
      " [0.2317743 ]\n",
      " [0.94262075]\n",
      " [0.15471676]\n",
      " [0.7825824 ]\n",
      " [0.9401607 ]\n",
      " [0.5915066 ]\n",
      " [0.22149494]\n",
      " [0.5402867 ]\n",
      " [0.7128022 ]\n",
      " [0.1607959 ]\n",
      " [0.28222102]\n",
      " [0.25462943]\n",
      " [0.7549982 ]\n",
      " [0.43485767]\n",
      " [0.75131917]\n",
      " [0.81058383]\n",
      " [0.8085191 ]\n",
      " [0.5854929 ]\n",
      " [0.6781777 ]\n",
      " [0.09256467]\n",
      " [0.6857417 ]\n",
      " [0.67171335]\n",
      " [0.3350553 ]\n",
      " [0.9441016 ]\n",
      " [0.57708097]\n",
      " [0.6586657 ]\n",
      " [0.69208556]\n",
      " [0.43932915]\n",
      " [0.95635414]\n",
      " [0.9037882 ]\n",
      " [0.6148026 ]\n",
      " [0.84330094]\n",
      " [0.3742635 ]\n",
      " [0.66419554]\n",
      " [0.82357264]\n",
      " [0.5348767 ]\n",
      " [0.41159254]\n",
      " [0.35956383]\n",
      " [0.86068606]\n",
      " [0.13898447]\n",
      " [0.40107733]\n",
      " [0.05502608]\n",
      " [0.5700681 ]\n",
      " [0.94349647]\n",
      " [0.7014698 ]\n",
      " [0.72075087]\n",
      " [0.950306  ]\n",
      " [0.93587   ]\n",
      " [0.9391782 ]\n",
      " [0.22009474]\n",
      " [0.3368981 ]\n",
      " [0.9675293 ]\n",
      " [0.17461532]\n",
      " [0.4689502 ]\n",
      " [0.13043347]\n",
      " [0.69007736]\n",
      " [0.8683833 ]\n",
      " [0.5014014 ]\n",
      " [0.95916975]\n",
      " [0.71636885]\n",
      " [0.658864  ]\n",
      " [0.8654946 ]\n",
      " [0.6325896 ]\n",
      " [0.5666381 ]\n",
      " [0.96354526]\n",
      " [0.7008175 ]\n",
      " [0.85853684]\n",
      " [0.6688812 ]\n",
      " [0.26099634]\n",
      " [0.72404706]\n",
      " [0.92727757]\n",
      " [0.93218637]\n",
      " [0.8942906 ]\n",
      " [0.7934079 ]\n",
      " [0.38311052]\n",
      " [0.8805554 ]\n",
      " [0.90419596]\n",
      " [0.92259794]\n",
      " [0.88277173]\n",
      " [0.8503173 ]\n",
      " [0.34206074]\n",
      " [0.8205401 ]\n",
      " [0.5313171 ]\n",
      " [0.86224747]\n",
      " [0.3915512 ]\n",
      " [0.9080657 ]\n",
      " [0.94859123]\n",
      " [0.77558863]\n",
      " [0.7842735 ]\n",
      " [0.6728348 ]\n",
      " [0.7308642 ]\n",
      " [0.55871475]\n",
      " [0.9071853 ]\n",
      " [0.9796234 ]\n",
      " [0.8950988 ]\n",
      " [0.55198693]\n",
      " [0.23205137]\n",
      " [0.6528459 ]\n",
      " [0.6713283 ]\n",
      " [0.9640628 ]\n",
      " [0.77377486]\n",
      " [0.7523117 ]\n",
      " [0.92018604]\n",
      " [0.66933346]\n",
      " [0.91982853]\n",
      " [0.8240049 ]\n",
      " [0.4659926 ]\n",
      " [0.33006924]\n",
      " [0.93569595]\n",
      " [0.88056314]\n",
      " [0.40283522]\n",
      " [0.4458319 ]\n",
      " [0.62911415]\n",
      " [0.8427459 ]\n",
      " [0.87975967]\n",
      " [0.93017113]\n",
      " [0.10951671]\n",
      " [0.7280313 ]\n",
      " [0.8548832 ]\n",
      " [0.6322085 ]\n",
      " [0.6491037 ]\n",
      " [0.7359294 ]\n",
      " [0.66565144]\n",
      " [0.82896984]\n",
      " [0.81304264]\n",
      " [0.6512787 ]\n",
      " [0.4884727 ]\n",
      " [0.41829062]\n",
      " [0.39352736]\n",
      " [0.78013384]\n",
      " [0.9450258 ]\n",
      " [0.81532216]\n",
      " [0.7929665 ]\n",
      " [0.86321026]\n",
      " [0.48308617]\n",
      " [0.7949458 ]\n",
      " [0.7637799 ]\n",
      " [0.72269654]\n",
      " [0.8741044 ]\n",
      " [0.6397538 ]\n",
      " [0.54732496]\n",
      " [0.7105953 ]\n",
      " [0.9200909 ]\n",
      " [0.76031506]\n",
      " [0.42991456]\n",
      " [0.93998325]\n",
      " [0.61525285]\n",
      " [0.80774444]\n",
      " [0.2655991 ]\n",
      " [0.37252212]\n",
      " [0.08483353]\n",
      " [0.20441175]\n",
      " [0.9146708 ]\n",
      " [0.885612  ]\n",
      " [0.9470784 ]\n",
      " [0.09785399]\n",
      " [0.5388765 ]\n",
      " [0.76154363]\n",
      " [0.58452636]\n",
      " [0.8697648 ]\n",
      " [0.4526848 ]\n",
      " [0.8154937 ]\n",
      " [0.6069941 ]\n",
      " [0.66031855]\n",
      " [0.73606104]\n",
      " [0.88001096]\n",
      " [0.7869938 ]\n",
      " [0.6079976 ]\n",
      " [0.89949656]\n",
      " [0.8654851 ]\n",
      " [0.95313394]\n",
      " [0.21010321]\n",
      " [0.8347313 ]\n",
      " [0.18728644]\n",
      " [0.33989137]\n",
      " [0.39580476]\n",
      " [0.90502715]\n",
      " [0.66353154]\n",
      " [0.925004  ]\n",
      " [0.9213524 ]\n",
      " [0.6073228 ]\n",
      " [0.12368381]\n",
      " [0.18056807]\n",
      " [0.63522464]\n",
      " [0.7598585 ]\n",
      " [0.6384636 ]\n",
      " [0.84552234]\n",
      " [0.6097366 ]\n",
      " [0.35359728]\n",
      " [0.16708234]\n",
      " [0.91093343]\n",
      " [0.3595511 ]\n",
      " [0.8740081 ]\n",
      " [0.91059417]\n",
      " [0.71806526]\n",
      " [0.6272913 ]\n",
      " [0.64341795]\n",
      " [0.54755026]\n",
      " [0.74077916]\n",
      " [0.9542365 ]\n",
      " [0.7484107 ]\n",
      " [0.83991295]\n",
      " [0.11055839]\n",
      " [0.30519933]\n",
      " [0.8989322 ]\n",
      " [0.19348666]\n",
      " [0.9414367 ]\n",
      " [0.26148057]\n",
      " [0.24019513]\n",
      " [0.4263418 ]\n",
      " [0.7107965 ]\n",
      " [0.18749738]\n",
      " [0.74525344]\n",
      " [0.7232901 ]\n",
      " [0.83777183]\n",
      " [0.6528271 ]\n",
      " [0.13551214]\n",
      " [0.34921452]\n",
      " [0.72765213]\n",
      " [0.50678986]\n",
      " [0.933217  ]\n",
      " [0.9375298 ]\n",
      " [0.70678514]\n",
      " [0.3358875 ]\n",
      " [0.0381867 ]\n",
      " [0.6186268 ]\n",
      " [0.34039128]\n",
      " [0.40442315]\n",
      " [0.9585308 ]\n",
      " [0.6347278 ]\n",
      " [0.9533632 ]\n",
      " [0.19534919]\n",
      " [0.12565908]\n",
      " [0.29104292]\n",
      " [0.8125679 ]\n",
      " [0.9163449 ]\n",
      " [0.8863717 ]\n",
      " [0.6634166 ]\n",
      " [0.68059313]\n",
      " [0.56159747]\n",
      " [0.15172267]\n",
      " [0.55938786]\n",
      " [0.1124149 ]\n",
      " [0.56947637]\n",
      " [0.8809152 ]\n",
      " [0.6709559 ]\n",
      " [0.7311481 ]\n",
      " [0.9574094 ]\n",
      " [0.818797  ]\n",
      " [0.7881322 ]\n",
      " [0.75508463]\n",
      " [0.7765064 ]\n",
      " [0.8674285 ]\n",
      " [0.37671727]\n",
      " [0.37707534]\n",
      " [0.52728987]\n",
      " [0.8327206 ]\n",
      " [0.63699734]\n",
      " [0.6828615 ]\n",
      " [0.8152713 ]\n",
      " [0.31959653]\n",
      " [0.47951156]\n",
      " [0.63838834]\n",
      " [0.6315419 ]\n",
      " [0.41386992]\n",
      " [0.90430063]\n",
      " [0.7922623 ]\n",
      " [0.93069035]\n",
      " [0.5554579 ]\n",
      " [0.7638254 ]\n",
      " [0.8307426 ]\n",
      " [0.8346149 ]\n",
      " [0.70319784]\n",
      " [0.87825406]\n",
      " [0.33511555]\n",
      " [0.5663222 ]\n",
      " [0.684031  ]\n",
      " [0.36205807]\n",
      " [0.8265554 ]\n",
      " [0.28361875]\n",
      " [0.58580446]\n",
      " [0.944525  ]\n",
      " [0.77492416]\n",
      " [0.8524026 ]\n",
      " [0.6854327 ]\n",
      " [0.46729457]\n",
      " [0.6111262 ]\n",
      " [0.39902964]\n",
      " [0.4350318 ]\n",
      " [0.6527683 ]\n",
      " [0.6484346 ]\n",
      " [0.64254624]\n",
      " [0.6562556 ]\n",
      " [0.2037197 ]\n",
      " [0.6658384 ]\n",
      " [0.9081174 ]\n",
      " [0.45867845]\n",
      " [0.6596938 ]\n",
      " [0.7431668 ]\n",
      " [0.46445566]\n",
      " [0.7206158 ]\n",
      " [0.52659476]\n",
      " [0.70826375]\n",
      " [0.9109984 ]\n",
      " [0.6538623 ]\n",
      " [0.6997264 ]\n",
      " [0.85943586]\n",
      " [0.5665692 ]\n",
      " [0.85088766]\n",
      " [0.95278025]\n",
      " [0.29719812]\n",
      " [0.768883  ]\n",
      " [0.24542928]\n",
      " [0.77778316]\n",
      " [0.820759  ]\n",
      " [0.71330434]\n",
      " [0.37541038]\n",
      " [0.77889186]\n",
      " [0.7417947 ]\n",
      " [0.7371796 ]\n",
      " [0.16675335]\n",
      " [0.7986509 ]\n",
      " [0.85017794]\n",
      " [0.6212884 ]\n",
      " [0.94109964]\n",
      " [0.20716986]\n",
      " [0.7343488 ]\n",
      " [0.95384514]\n",
      " [0.17904288]\n",
      " [0.47610542]\n",
      " [0.6970867 ]\n",
      " [0.32523277]\n",
      " [0.15809402]\n",
      " [0.85323834]\n",
      " [0.92544436]\n",
      " [0.867308  ]\n",
      " [0.6301335 ]\n",
      " [0.6637957 ]\n",
      " [0.55343556]\n",
      " [0.75984395]\n",
      " [0.83499205]\n",
      " [0.9417206 ]\n",
      " [0.73282707]\n",
      " [0.76202285]\n",
      " [0.6011614 ]\n",
      " [0.9409902 ]\n",
      " [0.9454746 ]\n",
      " [0.7208839 ]\n",
      " [0.2744174 ]\n",
      " [0.6806072 ]\n",
      " [0.32878596]\n",
      " [0.7555171 ]\n",
      " [0.18484002]\n",
      " [0.23503199]\n",
      " [0.40583766]\n",
      " [0.6961911 ]\n",
      " [0.34531564]\n",
      " [0.55344385]\n",
      " [0.8379798 ]\n",
      " [0.6753847 ]\n",
      " [0.869926  ]\n",
      " [0.9552452 ]\n",
      " [0.7567663 ]\n",
      " [0.09640157]\n",
      " [0.47172752]\n",
      " [0.8342231 ]\n",
      " [0.852174  ]\n",
      " [0.66084385]\n",
      " [0.26253057]\n",
      " [0.89826417]\n",
      " [0.89355415]\n",
      " [0.25118858]\n",
      " [0.62142396]\n",
      " [0.85081303]\n",
      " [0.874721  ]\n",
      " [0.8732128 ]\n",
      " [0.91738045]\n",
      " [0.8823082 ]\n",
      " [0.9236567 ]\n",
      " [0.69492924]\n",
      " [0.6066545 ]\n",
      " [0.54264796]\n",
      " [0.8452531 ]\n",
      " [0.8771823 ]\n",
      " [0.20067099]\n",
      " [0.8169886 ]\n",
      " [0.88712907]\n",
      " [0.33379298]\n",
      " [0.6465434 ]\n",
      " [0.878037  ]\n",
      " [0.54932857]\n",
      " [0.9320483 ]\n",
      " [0.2473462 ]\n",
      " [0.841157  ]\n",
      " [0.61173314]\n",
      " [0.8906881 ]\n",
      " [0.33569902]\n",
      " [0.65808475]\n",
      " [0.73417974]\n",
      " [0.8233528 ]\n",
      " [0.1047453 ]\n",
      " [0.20024484]\n",
      " [0.7057992 ]\n",
      " [0.8201492 ]\n",
      " [0.44638082]\n",
      " [0.781613  ]\n",
      " [0.46114802]\n",
      " [0.35070044]\n",
      " [0.8700203 ]\n",
      " [0.44304258]\n",
      " [0.94038296]\n",
      " [0.820017  ]\n",
      " [0.6456575 ]\n",
      " [0.9226796 ]\n",
      " [0.63451695]\n",
      " [0.7971128 ]\n",
      " [0.29987067]\n",
      " [0.2535479 ]\n",
      " [0.7624711 ]\n",
      " [0.3872305 ]\n",
      " [0.45175144]\n",
      " [0.89623284]\n",
      " [0.9124452 ]\n",
      " [0.91624635]\n",
      " [0.95468247]\n",
      " [0.7109363 ]\n",
      " [0.9096731 ]\n",
      " [0.33134976]\n",
      " [0.35355616]\n",
      " [0.4938761 ]\n",
      " [0.9505924 ]\n",
      " [0.6191799 ]\n",
      " [0.16807088]\n",
      " [0.9325811 ]\n",
      " [0.80928314]\n",
      " [0.60931677]\n",
      " [0.80953574]\n",
      " [0.01596662]\n",
      " [0.9286801 ]\n",
      " [0.77519965]\n",
      " [0.75772524]\n",
      " [0.7684462 ]\n",
      " [0.9702468 ]\n",
      " [0.65548384]\n",
      " [0.76919043]\n",
      " [0.756334  ]\n",
      " [0.8451154 ]\n",
      " [0.17446691]\n",
      " [0.6250803 ]\n",
      " [0.91497123]\n",
      " [0.5971357 ]\n",
      " [0.7712045 ]\n",
      " [0.9554055 ]\n",
      " [0.85055465]\n",
      " [0.897156  ]\n",
      " [0.59040916]\n",
      " [0.7906256 ]\n",
      " [0.94578344]\n",
      " [0.7465949 ]\n",
      " [0.6483718 ]\n",
      " [0.27441058]\n",
      " [0.4522631 ]\n",
      " [0.5306363 ]\n",
      " [0.6044551 ]\n",
      " [0.5397597 ]\n",
      " [0.79117435]\n",
      " [0.58466023]\n",
      " [0.7916827 ]\n",
      " [0.8464211 ]\n",
      " [0.74654084]\n",
      " [0.66084415]\n",
      " [0.46457714]\n",
      " [0.59682643]\n",
      " [0.93893456]\n",
      " [0.8454697 ]\n",
      " [0.22788173]\n",
      " [0.40788606]\n",
      " [0.463023  ]\n",
      " [0.08746302]\n",
      " [0.9019096 ]\n",
      " [0.1444656 ]\n",
      " [0.9004141 ]\n",
      " [0.8852403 ]\n",
      " [0.8382101 ]\n",
      " [0.69302297]\n",
      " [0.89649224]\n",
      " [0.35038343]\n",
      " [0.7979303 ]\n",
      " [0.9442509 ]\n",
      " [0.2849509 ]\n",
      " [0.4410699 ]\n",
      " [0.88151085]\n",
      " [0.87658966]\n",
      " [0.6529148 ]\n",
      " [0.8126795 ]\n",
      " [0.8131187 ]\n",
      " [0.82350826]\n",
      " [0.2507968 ]\n",
      " [0.7561747 ]\n",
      " [0.89957047]\n",
      " [0.64478993]\n",
      " [0.8084915 ]\n",
      " [0.7119852 ]\n",
      " [0.82963705]\n",
      " [0.88336337]\n",
      " [0.93458027]\n",
      " [0.5818506 ]\n",
      " [0.41947424]\n",
      " [0.78521526]\n",
      " [0.7864673 ]\n",
      " [0.97185135]\n",
      " [0.7670039 ]\n",
      " [0.69854474]\n",
      " [0.4106228 ]\n",
      " [0.7170478 ]\n",
      " [0.93292797]\n",
      " [0.95555276]\n",
      " [0.8991205 ]\n",
      " [0.7098373 ]\n",
      " [0.69884175]\n",
      " [0.8118403 ]\n",
      " [0.46381426]\n",
      " [0.80929947]\n",
      " [0.8160162 ]\n",
      " [0.9019245 ]\n",
      " [0.61354977]\n",
      " [0.72936   ]\n",
      " [0.9137919 ]\n",
      " [0.49849498]\n",
      " [0.543336  ]\n",
      " [0.6507154 ]\n",
      " [0.72941744]\n",
      " [0.67678297]\n",
      " [0.9032401 ]\n",
      " [0.9271891 ]\n",
      " [0.19509351]\n",
      " [0.11248124]\n",
      " [0.75000846]\n",
      " [0.49522722]\n",
      " [0.25213504]\n",
      " [0.8446383 ]\n",
      " [0.91003263]\n",
      " [0.7126718 ]\n",
      " [0.93890965]\n",
      " [0.91425467]\n",
      " [0.7711783 ]\n",
      " [0.8365573 ]\n",
      " [0.70891595]\n",
      " [0.5234985 ]\n",
      " [0.7881615 ]\n",
      " [0.5987449 ]\n",
      " [0.09917179]\n",
      " [0.90080273]\n",
      " [0.88727635]\n",
      " [0.74616104]\n",
      " [0.92461586]\n",
      " [0.8503685 ]\n",
      " [0.87938106]\n",
      " [0.56903434]\n",
      " [0.6799412 ]\n",
      " [0.8929941 ]\n",
      " [0.75972843]\n",
      " [0.85817885]\n",
      " [0.90590453]\n",
      " [0.6168813 ]\n",
      " [0.78432786]\n",
      " [0.8382336 ]\n",
      " [0.52630705]\n",
      " [0.54393196]\n",
      " [0.0847064 ]\n",
      " [0.23812646]\n",
      " [0.84695375]\n",
      " [0.6513821 ]\n",
      " [0.67066026]\n",
      " [0.5794535 ]\n",
      " [0.9456674 ]\n",
      " [0.4212637 ]\n",
      " [0.83437854]\n",
      " [0.26372093]\n",
      " [0.91772366]\n",
      " [0.32448745]\n",
      " [0.7662241 ]\n",
      " [0.57374066]\n",
      " [0.878538  ]\n",
      " [0.5899048 ]\n",
      " [0.23155522]\n",
      " [0.7795833 ]\n",
      " [0.93313783]\n",
      " [0.3483702 ]\n",
      " [0.9225918 ]\n",
      " [0.88833797]\n",
      " [0.8701257 ]\n",
      " [0.8207637 ]\n",
      " [0.39610544]\n",
      " [0.30606955]\n",
      " [0.67553365]\n",
      " [0.1738536 ]\n",
      " [0.9576782 ]\n",
      " [0.31811827]\n",
      " [0.9301862 ]\n",
      " [0.8794585 ]\n",
      " [0.38828683]\n",
      " [0.19519562]\n",
      " [0.72347355]\n",
      " [0.4145139 ]\n",
      " [0.8534665 ]\n",
      " [0.73645145]\n",
      " [0.9826927 ]\n",
      " [0.58842826]\n",
      " [0.63849723]\n",
      " [0.77828205]\n",
      " [0.8445878 ]\n",
      " [0.07118461]\n",
      " [0.71842086]\n",
      " [0.8058318 ]\n",
      " [0.8459811 ]\n",
      " [0.6509381 ]\n",
      " [0.46763003]\n",
      " [0.59697753]\n",
      " [0.9113097 ]\n",
      " [0.66184044]\n",
      " [0.78137904]\n",
      " [0.8330882 ]\n",
      " [0.8665838 ]\n",
      " [0.82268286]\n",
      " [0.59284115]\n",
      " [0.80208516]\n",
      " [0.906754  ]\n",
      " [0.69656146]\n",
      " [0.96267056]\n",
      " [0.8173982 ]\n",
      " [0.6228927 ]\n",
      " [0.49098775]\n",
      " [0.8489398 ]\n",
      " [0.8633039 ]\n",
      " [0.4478323 ]\n",
      " [0.66281736]\n",
      " [0.20587721]\n",
      " [0.572172  ]\n",
      " [0.8264458 ]\n",
      " [0.9511798 ]\n",
      " [0.82751954]\n",
      " [0.72256553]\n",
      " [0.7697873 ]\n",
      " [0.88513845]\n",
      " [0.46372274]\n",
      " [0.9377962 ]\n",
      " [0.5697267 ]\n",
      " [0.862792  ]\n",
      " [0.31570545]\n",
      " [0.07876313]\n",
      " [0.26706153]\n",
      " [0.33203983]\n",
      " [0.6933455 ]\n",
      " [0.8275614 ]\n",
      " [0.5692553 ]\n",
      " [0.7551786 ]\n",
      " [0.8047435 ]\n",
      " [0.48192662]\n",
      " [0.3496961 ]\n",
      " [0.90765804]\n",
      " [0.8989004 ]\n",
      " [0.36171365]\n",
      " [0.67684317]\n",
      " [0.1758588 ]\n",
      " [0.40807438]\n",
      " [0.74940693]\n",
      " [0.6937894 ]\n",
      " [0.9125596 ]\n",
      " [0.98054075]\n",
      " [0.16193718]\n",
      " [0.7006083 ]\n",
      " [0.6290908 ]\n",
      " [0.43660027]\n",
      " [0.724881  ]\n",
      " [0.7588181 ]\n",
      " [0.89550906]\n",
      " [0.7498541 ]\n",
      " [0.43829417]\n",
      " [0.70300007]\n",
      " [0.15272248]\n",
      " [0.66948986]\n",
      " [0.5025882 ]\n",
      " [0.9222305 ]\n",
      " [0.5773008 ]\n",
      " [0.55034703]\n",
      " [0.8178874 ]\n",
      " [0.72681713]\n",
      " [0.4555756 ]\n",
      " [0.74850357]\n",
      " [0.6652864 ]\n",
      " [0.31877923]\n",
      " [0.58202666]\n",
      " [0.8889682 ]\n",
      " [0.84271777]\n",
      " [0.5923221 ]\n",
      " [0.7707548 ]\n",
      " [0.27885622]\n",
      " [0.84441864]\n",
      " [0.5849954 ]\n",
      " [0.75665665]\n",
      " [0.38638836]\n",
      " [0.650123  ]\n",
      " [0.8470633 ]\n",
      " [0.15438291]\n",
      " [0.28971884]\n",
      " [0.83006835]\n",
      " [0.811471  ]\n",
      " [0.7916341 ]\n",
      " [0.91874444]\n",
      " [0.76604164]\n",
      " [0.68441594]\n",
      " [0.7116432 ]\n",
      " [0.7833729 ]\n",
      " [0.68740875]\n",
      " [0.79222596]\n",
      " [0.50442207]\n",
      " [0.4772954 ]\n",
      " [0.89100695]\n",
      " [0.7989304 ]\n",
      " [0.67733264]\n",
      " [0.25317964]\n",
      " [0.8833293 ]\n",
      " [0.8375337 ]\n",
      " [0.83536935]\n",
      " [0.6790209 ]\n",
      " [0.89642614]\n",
      " [0.8544681 ]\n",
      " [0.7646173 ]\n",
      " [0.39442748]\n",
      " [0.8895263 ]\n",
      " [0.9123826 ]\n",
      " [0.34858805]\n",
      " [0.15339816]\n",
      " [0.7396829 ]\n",
      " [0.3658592 ]\n",
      " [0.79005444]\n",
      " [0.28766322]\n",
      " [0.4489925 ]\n",
      " [0.46092492]\n",
      " [0.757133  ]\n",
      " [0.88048565]\n",
      " [0.12682611]\n",
      " [0.3708548 ]\n",
      " [0.61026895]\n",
      " [0.5133806 ]\n",
      " [0.5025651 ]\n",
      " [0.7892549 ]\n",
      " [0.14264098]\n",
      " [0.9233888 ]\n",
      " [0.15704769]\n",
      " [0.870719  ]\n",
      " [0.7154367 ]\n",
      " [0.72279805]\n",
      " [0.8363673 ]\n",
      " [0.7123763 ]\n",
      " [0.9048562 ]] \n",
      "Correct(Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.76943344\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis>0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict=feed)\n",
    "        if step %200 ==0:\n",
    "            print(step, sess.run(cost, feed_dict = feed))\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict=feed)\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect(Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 06 Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.414217\n",
      "200 0.65463257\n",
      "400 0.54534787\n",
      "600 0.45133916\n",
      "800 0.36068386\n",
      "1000 0.27280414\n",
      "1200 0.230899\n",
      "1400 0.21000385\n",
      "1600 0.19243923\n",
      "1800 0.17747521\n",
      "2000 0.16458377\n",
      "--------------\n",
      "[[3.2684024e-02 9.6730608e-01 9.9438948e-06]] [1]\n",
      "--------------\n",
      "[[0.67285097 0.2973502  0.02979887]] [0]\n",
      "--------------\n",
      "[[1.7489613e-08 4.0298089e-04 9.9959701e-01]] [2]\n",
      "--------------\n",
      "[[3.2684054e-02 9.6730608e-01 9.9438848e-06]\n",
      " [6.7285097e-01 2.9735020e-01 2.9798875e-02]\n",
      " [1.7489613e-08 4.0298089e-04 9.9959701e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "x_data= [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7]]\n",
    "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W)+b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict= {X:x_data, Y:y_data})\n",
    "        if step%200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}))\n",
    "    print('--------------')\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06-2 Fancy Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('DeepLearningZeroToAll-master/data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot: Tensor(\"one_hot_4:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape one_hot: Tensor(\"Reshape_4:0\", shape=(?, 7), dtype=float32)\n",
      "Step:     0\tLoss: 4.482080\tAcc: 14.85%\n",
      "Step:   100\tLoss: 0.612605\tAcc: 80.20%\n",
      "Step:   200\tLoss: 0.383739\tAcc: 89.11%\n",
      "Step:   300\tLoss: 0.282933\tAcc: 93.07%\n",
      "Step:   400\tLoss: 0.225934\tAcc: 95.05%\n",
      "Step:   500\tLoss: 0.188747\tAcc: 96.04%\n",
      "Step:   600\tLoss: 0.162239\tAcc: 96.04%\n",
      "Step:   700\tLoss: 0.142244\tAcc: 96.04%\n",
      "Step:   800\tLoss: 0.126574\tAcc: 98.02%\n",
      "Step:   900\tLoss: 0.113955\tAcc: 99.01%\n",
      "Step:  1000\tLoss: 0.103582\tAcc: 99.01%\n",
      "Step:  1100\tLoss: 0.094912\tAcc: 99.01%\n",
      "Step:  1200\tLoss: 0.087568\tAcc: 100.00%\n",
      "Step:  1300\tLoss: 0.081273\tAcc: 100.00%\n",
      "Step:  1400\tLoss: 0.075822\tAcc: 100.00%\n",
      "Step:  1500\tLoss: 0.071059\tAcc: 100.00%\n",
      "Step:  1600\tLoss: 0.066863\tAcc: 100.00%\n",
      "Step:  1700\tLoss: 0.063141\tAcc: 100.00%\n",
      "Step:  1800\tLoss: 0.059817\tAcc: 100.00%\n",
      "Step:  1900\tLoss: 0.056831\tAcc: 100.00%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot:\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape one_hot:\", Y_one_hot)\n",
    "\n",
    "'''\n",
    "one_hot: Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
    "reshape one_hot: Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
    "'''\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                 labels=tf.stop_gradient([Y_one_hot])))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "            print (\"Step: {:5}\\tLoss: {:3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n",
    "            \n",
    "    pred = sess.run(prediction, feed_dict = {X: x_data})\n",
    "    \n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p==int(y), p, int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
