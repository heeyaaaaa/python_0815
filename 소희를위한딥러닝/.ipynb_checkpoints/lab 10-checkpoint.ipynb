{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-eef628746518>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  001, Cost: 5.745170948\n",
      "Epoch:  002, Cost: 1.780056695\n",
      "Epoch:  003, Cost: 1.122778636\n",
      "Epoch:  004, Cost: 0.872012249\n",
      "Epoch:  005, Cost: 0.738203182\n",
      "Epoch:  006, Cost: 0.654728894\n",
      "Epoch:  007, Cost: 0.596023606\n",
      "Epoch:  008, Cost: 0.552216825\n",
      "Epoch:  009, Cost: 0.518254966\n",
      "Epoch:  010, Cost: 0.491113195\n",
      "Epoch:  011, Cost: 0.468347532\n",
      "Epoch:  012, Cost: 0.449374351\n",
      "Epoch:  013, Cost: 0.432675663\n",
      "Epoch:  014, Cost: 0.418828156\n",
      "Epoch:  015, Cost: 0.406128936\n",
      "Epoch:  016, Cost: 0.394982944\n",
      "Epoch:  017, Cost: 0.385870417\n",
      "Epoch:  018, Cost: 0.376135584\n",
      "Epoch:  019, Cost: 0.368269379\n",
      "Epoch:  020, Cost: 0.361209774\n",
      "Epoch:  021, Cost: 0.354798142\n",
      "Epoch:  022, Cost: 0.348525120\n",
      "Epoch:  023, Cost: 0.342752729\n",
      "Epoch:  024, Cost: 0.337285910\n",
      "Epoch:  025, Cost: 0.332443600\n",
      "Epoch:  026, Cost: 0.327556536\n",
      "Epoch:  027, Cost: 0.324047233\n",
      "Epoch:  028, Cost: 0.319670901\n",
      "Epoch:  029, Cost: 0.315536206\n",
      "Epoch:  030, Cost: 0.312257757\n",
      "Epoch:  031, Cost: 0.308550813\n",
      "Epoch:  032, Cost: 0.305987610\n",
      "Epoch:  033, Cost: 0.302624451\n",
      "Epoch:  034, Cost: 0.299895896\n",
      "Epoch:  035, Cost: 0.297245876\n",
      "Epoch:  036, Cost: 0.294490167\n",
      "Epoch:  037, Cost: 0.292061209\n",
      "Epoch:  038, Cost: 0.290009242\n",
      "Epoch:  039, Cost: 0.287633527\n",
      "Epoch:  040, Cost: 0.285644498\n",
      "Epoch:  041, Cost: 0.283856602\n",
      "Epoch:  042, Cost: 0.281824820\n",
      "Epoch:  043, Cost: 0.280098968\n",
      "Epoch:  044, Cost: 0.278386739\n",
      "Epoch:  045, Cost: 0.276589554\n",
      "Epoch:  046, Cost: 0.275093700\n",
      "Epoch:  047, Cost: 0.273444048\n",
      "Epoch:  048, Cost: 0.271918683\n",
      "Epoch:  049, Cost: 0.270640440\n",
      "Epoch:  050, Cost: 0.269054373\n",
      "Learning Finished!\n",
      "Accuracy: 0.9194\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAO9ElEQVR4nO3dfYxUZZbH8d8RaYk4Km63LpF2GSckvmDWMS0RISNmlCgxKH8MGVFkoxmMihnNoIts4hhijFlhJhrXSXgHMzLRjCh/GB3FEUOixNIwiOAKa9oRIdIE4zi+ZBb77B9dbFrs+9yy6lbdwvP9JJ2quqeeuocbfn2r66mqx9xdAL7/jim7AQCtQdiBIAg7EARhB4Ig7EAQx7ZyZ52dnT527NhW7hIIpbe3VwcOHLChag2F3cyukPSwpGGSlrv7g6n7jx07VpVKpZFdAkjo6enJrNX9NN7Mhkn6L0lXSjpH0rVmdk69jweguRr5m32CpN3u/r67/0PSHyRdXUxbAIrWSNhPl/ThoNt7qtu+wczmmlnFzCp9fX0N7A5AIxoJ+1AvAnzrvbfuvtTde9y9p6urq4HdAWhEI2HfI6l70O0xkvY21g6AZmkk7G9IGmdmPzSzDkk/l7ShmLYAFK3uqTd3P2Rm8yS9oIGpt5Xu/k5hnQEoVEPz7O7+nKTnCuoFQBPxdlkgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEA0t2WxmvZI+k/S1pEPu3lNEUwCK11DYqy519wMFPA6AJuJpPBBEo2F3SX8yszfNbO5QdzCzuWZWMbNKX19fg7sDUK9Gwz7J3S+QdKWk28zsJ0fewd2XunuPu/d0dXU1uDsA9Woo7O6+t3q5X9J6SROKaApA8eoOu5mNNLMfHL4uaaqk7UU1BqBYjbwaf5qk9WZ2+HGecPfnC+kK33DgQHqyY+PGjZm1u+++Ozn2ww8/rKunWs2cOTOzduGFFybHzpgxI1k/88wz6+opqrrD7u7vS/rXAnsB0ERMvQFBEHYgCMIOBEHYgSAIOxBEER+EQYMeffTRZH3hwoXJ+ueff15kO98wYUL6fVIdHR3J+t69ezNrd911V3Js3r/7zjvvTNbnz5+fWevs7EyO/T7izA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQTDPXoD+/v5kfcGCBcn64sWLk/WTTz45WT/ppJMya59++mly7Lhx45L1TZs2JevHHXdcsp6yYcOGZP3mm29O1h966KG6H/+FF15Iju3u7k7Wj0ac2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCObZC7B8+fJkfcmSJcn6GWeckaw/++yzyXpqvnndunXJsWvWrEnWG5lHzzN9+vRk/eKLL07W582bl6w/9dRTmbW8r7Hu7e1N1keMGJGstyPO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBPPsBcibq3b3ZH316tXJet6c7xNPPJFZu+OOO5JjL7roomS9THnf7f74448n66njtmXLluTYp59+OlmfNWtWst6Ocs/sZrbSzPab2fZB204xsxfNbFf1clRz2wTQqFqexq+WdMUR2xZI2uju4yRtrN4G0MZyw+7ur0o6eMTmqyUdfu66RtI1BfcFoGD1vkB3mrvvk6Tq5alZdzSzuWZWMbNKX19fnbsD0Kimvxrv7kvdvcfde7q6upq9OwAZ6g37x2Y2WpKql/uLawlAM9Qb9g2S5lSvz5GU/gwmgNLlzrOb2TpJUyR1mtkeSb+W9KCkJ83sJkl/lfSzZjbZDnbv3p1Zq1QqybGXXnppsp63Bvr48eOTdTPLrE2cODE59mg2fPjwZD31efYLLrggOfbee+9N1vM+i3/CCSck62XIDbu7X5tR+mnBvQBoIt4uCwRB2IEgCDsQBGEHgiDsQBB8xLVGixYtyqwdOnQoOfb+++9P1nfu3Jmsf/DBB8l6ahoob4ro+yy17PLs2bOTYx9++OFkfevWrcn65MmTk/UycGYHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSCYZ6/RK6+8UvfYE088MVlfsWJF3Y8tSZMmTcqsNXPJ5aPZVVddlaznzbO//vrryTrz7ABKQ9iBIAg7EARhB4Ig7EAQhB0IgrADQTDPXqP+/v7MWt6SzHnyxjdax7eNGzcuWe/o6EjWH3nkkWR9/vz537mnZuPMDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM9eo2OOyf69mFoyuRZ54/PqM2fObGj/EaW+U16S7rnnnmR9+fLlRbbTErlndjNbaWb7zWz7oG33mdlHZra1+jOtuW0CaFQtT+NXS7piiO2/dffzqz/PFdsWgKLlht3dX5V0sAW9AGiiRl6gm2dm26pP80dl3cnM5ppZxcwqfX19DewOQCPqDfvvJP1I0vmS9klaknVHd1/q7j3u3tPV1VXn7gA0qq6wu/vH7v61u/dLWiZpQrFtAShaXWE3s9GDbs6QtD3rvgDaQ+48u5mtkzRFUqeZ7ZH0a0lTzOx8SS6pV9LNTewROYYPH152C987zzzzTLI+ceLEFnVSnNywu/u1Q2xubFUDAC3H22WBIAg7EARhB4Ig7EAQhB0Igo+41mj8+PGZtY8++qiFnaAIr732WrL+7rvvJuuzZs0qsp2W4MwOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewz16j2bNnZ9aef/75hh6bJZlb78knn0zWv/rqq2R9zJgxRbbTEpzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAI5tlrdPnll2fWRo4cmRxbqVSS9bwlmfMev6OjI1mPaseOHZm1ZcuWJcdOmJBe92T69Ol19VQmzuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATz7DXq7OzMrN1yyy3JsWvXrk3W169fn6znfcd51CWbv/zyy2T9uuuuy6z19/cnx+bNwx9//PHJejvKPbObWbeZ/dnMdprZO2b2y+r2U8zsRTPbVb0c1fx2AdSrlqfxhyT9yt3PlnSRpNvM7BxJCyRtdPdxkjZWbwNoU7lhd/d97v5W9fpnknZKOl3S1ZLWVO+2RtI1zWoSQOO+0wt0ZjZW0o8lbZF0mrvvkwZ+IUg6NWPMXDOrmFmlr6+vsW4B1K3msJvZCZL+KOkOd/9brePcfam797h7T1dXVz09AihATWE3s+EaCPrv3f3p6uaPzWx0tT5a0v7mtAigCLlTbzbw+csVkna6+28GlTZImiPpwerls03p8CjwwAMPJOtTp05N1jdv3pys5y0JnZommj9/fnJsO8v7OucZM2Yk69u2bcusrVq1Kjn2vPPOS9aPRrXMs0+SNFvS22a2tbptoQZC/qSZ3STpr5J+1pwWARQhN+zuvllS1rcr/LTYdgA0C2+XBYIg7EAQhB0IgrADQRB2IAg+4lqAY49NH8aXX345Wf/kk0+S9V27diXrixYtyqw99thjybG33nprst5MeR9RnTZtWrK+adOmZH3KlCmZtRtuuCE59vuIMzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME8exsYNSr9xbxffPFFsp6ab77sssuSY9etW5esX3LJJcl63nLS7733XmYtbx794MGDyfrkyZOT9byv6I6GMzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME8+1Egb3ngl156KbOWt+TW2WefnaznfeY8tZS1JB04cCCz5u7Jsbfffnuyvnjx4mQ96lLWWTizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQtazP3i1praR/ltQvaam7P2xm90n6haTDE7kL3f25ZjWKbCNGjMisdXd3J8du2bIlWc+by96xY0eyfv3112fWbrzxxuTYs846K1kfNmxYso5vquVNNYck/crd3zKzH0h608xerNZ+6+7p/w0A2kIt67Pvk7Svev0zM9sp6fRmNwagWN/pb3YzGyvpx5IOP/ebZ2bbzGylmQ353UpmNtfMKmZWyXvrJoDmqTnsZnaCpD9KusPd/ybpd5J+JOl8DZz5lww1zt2XunuPu/d0dXUV0DKAetQUdjMbroGg/97dn5Ykd//Y3b92935JyyRNaF6bABqVG3YzM0krJO10998M2j560N1mSNpefHsAilLLq/GTJM2W9LaZba1uWyjpWjM7X5JL6pV0c1M6RFOde+65yfqqVata1AmarZZX4zdLsiFKzKkDRxHeQQcEQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQjC8pbNLXRnZn2SPhi0qVNS9pq+5WrX3tq1L4ne6lVkb//i7kN+/1tLw/6tnZtV3L2ntAYS2rW3du1Lord6tao3nsYDQRB2IIiyw7605P2ntGtv7dqXRG/1aklvpf7NDqB1yj6zA2gRwg4EUUrYzewKM/tvM9ttZgvK6CGLmfWa2dtmttXMKiX3stLM9pvZ9kHbTjGzF81sV/VyyDX2SurtPjP7qHrstprZtJJ66zazP5vZTjN7x8x+Wd1e6rFL9NWS49byv9nNbJik9yRdLmmPpDckXevu6YW+W8TMeiX1uHvpb8Aws59I+rukte4+vrrtPyUddPcHq78oR7n7v7dJb/dJ+nvZy3hXVysaPXiZcUnXSPo3lXjsEn3NVAuOWxln9gmSdrv7++7+D0l/kHR1CX20PXd/VdLBIzZfLWlN9foaDfxnabmM3tqCu+9z97eq1z+TdHiZ8VKPXaKvligj7KdL+nDQ7T1qr/XeXdKfzOxNM5tbdjNDOM3d90kD/3kknVpyP0fKXca7lY5YZrxtjl09y583qoywD7WUVDvN/01y9wskXSnpturTVdSmpmW8W2WIZcbbQr3LnzeqjLDvkdQ96PYYSXtL6GNI7r63erlf0nq131LUHx9eQbd6ub/kfv5fOy3jPdQy42qDY1fm8udlhP0NSePM7Idm1iHp55I2lNDHt5jZyOoLJzKzkZKmqv2Wot4gaU71+hxJz5bYyze0yzLeWcuMq+RjV/ry5+7e8h9J0zTwivz/SPqPMnrI6OtMSX+p/rxTdm+S1mngad3/auAZ0U2S/knSRkm7qpentFFvj0t6W9I2DQRrdEm9TdbAn4bbJG2t/kwr+9gl+mrJcePtskAQvIMOCIKwA0EQdiAIwg4EQdiBIAg7EARhB4L4PzEQciqrfb4FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "logits = hypothesis, labels = tf.stop_gradient(Y)))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis =1), tf.argmax(Y, axis =1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict = {X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "            \n",
    "        print(f'Epoch: {(epoch+1): 04d}, Cost: {avg_cost:.9f}' )\n",
    "    print('Learning Finished!')\n",
    "    \n",
    "    print(\n",
    "    'Accuracy:',\n",
    "    sess.run(accuracy, feed_dict = {X: mnist.test.images, Y:mnist.test.labels}),)\n",
    "    \n",
    "    r = random.randint(0, mnist.test.num_examples -1)\n",
    "    \n",
    "    print('Label: ', sess.run(tf.argmax(mnist.test.labels[r:r+1], axis =1)))\n",
    "    print(\n",
    "    'Prediction: ',\n",
    "    sess.run(tf.argmax(hypothesis, axis =1), feed_dict = {X: mnist.test.images[r:r+1]}),)\n",
    "    \n",
    "    plt.imshow(\n",
    "    mnist.test.images[r:r+1].reshape(28,28),\n",
    "    cmap = 'Greys',\n",
    "    interpolation = 'nearest',\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-cb0f40e085e1>:26: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost = 165.594522565\n",
      "Epoch: 0002 cost = 41.138669815\n",
      "Epoch: 0003 cost = 25.814799466\n",
      "Epoch: 0004 cost = 17.958285316\n",
      "Epoch: 0005 cost = 12.953857318\n",
      "Epoch: 0006 cost = 9.632919692\n",
      "Epoch: 0007 cost = 7.189394053\n",
      "Epoch: 0008 cost = 5.342478854\n",
      "Epoch: 0009 cost = 4.013784026\n",
      "Epoch: 0010 cost = 3.033580145\n",
      "Epoch: 0011 cost = 2.195255387\n",
      "Epoch: 0012 cost = 1.776146657\n",
      "Epoch: 0013 cost = 1.187301824\n",
      "Epoch: 0014 cost = 0.961702899\n",
      "Epoch: 0015 cost = 0.735076855\n",
      "Learning Finished!\n",
      "Accuracy: 0.9448\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    }
   ],
   "source": [
    "## NN\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Epoch: 0001 cost = 0.307452268\n",
      "Epoch: 0002 cost = 0.119914374\n",
      "Epoch: 0003 cost = 0.080181229\n",
      "Epoch: 0004 cost = 0.056366151\n",
      "Epoch: 0005 cost = 0.041144067\n",
      "Epoch: 0006 cost = 0.031003116\n",
      "Epoch: 0007 cost = 0.026117952\n",
      "Epoch: 0008 cost = 0.019666148\n",
      "Epoch: 0009 cost = 0.017771796\n",
      "Epoch: 0010 cost = 0.012618105\n",
      "Epoch: 0011 cost = 0.013037014\n",
      "Epoch: 0012 cost = 0.012333069\n",
      "Epoch: 0013 cost = 0.010483045\n",
      "Epoch: 0014 cost = 0.008995165\n",
      "Epoch: 0015 cost = 0.009287994\n",
      "Learning Finished!\n",
      "Accuracy: 0.9774\n",
      "Label:  [5]\n",
      "Prediction:  [5]\n"
     ]
    }
   ],
   "source": [
    "## Xavier\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-03bd18a79b68>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-03bd18a79b68>:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost = 0.296018694\n",
      "Epoch: 0002 cost = 0.104580899\n",
      "Epoch: 0003 cost = 0.070556037\n",
      "Epoch: 0004 cost = 0.051803492\n",
      "Epoch: 0005 cost = 0.039884711\n",
      "Epoch: 0006 cost = 0.034317403\n",
      "Epoch: 0007 cost = 0.030248818\n",
      "Epoch: 0008 cost = 0.027117671\n",
      "Epoch: 0009 cost = 0.023098451\n",
      "Epoch: 0010 cost = 0.022976980\n",
      "Epoch: 0011 cost = 0.017514637\n",
      "Epoch: 0012 cost = 0.016424562\n",
      "Epoch: 0013 cost = 0.015816051\n",
      "Epoch: 0014 cost = 0.016863760\n",
      "Epoch: 0015 cost = 0.015272453\n",
      "Learning Finished!\n",
      "Accuracy: 0.98\n",
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-6ea4bfea3966>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/sohee/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-6ea4bfea3966>:31: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-6ea4bfea3966>:58: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost = 0.468228150\n",
      "Epoch: 0002 cost = 0.169418661\n",
      "Epoch: 0003 cost = 0.130039404\n",
      "Epoch: 0004 cost = 0.105761018\n",
      "Epoch: 0005 cost = 0.089331586\n",
      "Epoch: 0006 cost = 0.083918068\n",
      "Epoch: 0007 cost = 0.075034437\n",
      "Epoch: 0008 cost = 0.065880406\n",
      "Epoch: 0009 cost = 0.064087425\n",
      "Epoch: 0010 cost = 0.060889446\n",
      "Epoch: 0011 cost = 0.052671721\n",
      "Epoch: 0012 cost = 0.051312211\n",
      "Epoch: 0013 cost = 0.049161431\n",
      "Epoch: 0014 cost = 0.048996049\n",
      "Epoch: 0015 cost = 0.046989990\n",
      "Learning Finished!\n",
      "Accuracy: 0.9802\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
