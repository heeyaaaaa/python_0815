{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 07-1 Learning rate, Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.73203 [[ 0.7288166   0.7153621  -1.1801533 ]\n",
      " [-0.5775373  -0.1298833   1.6072978 ]\n",
      " [ 0.48373488 -0.51433605 -2.02127   ]]\n",
      "1 3.317995 [[ 0.6621908   0.7479632  -1.1461285 ]\n",
      " [-0.81948906  0.03000022  1.689366  ]\n",
      " [ 0.23214605 -0.33772916 -1.9462881 ]]\n",
      "2 2.0218027 [[ 0.6434202   0.74127686 -1.1206716 ]\n",
      " [-0.8116129  -0.00900118  1.7204912 ]\n",
      " [ 0.20866646 -0.3507957  -1.909742  ]]\n",
      "3 1.9710886 [[ 0.6235321   0.7400824  -1.099589  ]\n",
      " [-0.80967706 -0.0163628   1.725917  ]\n",
      " [ 0.17870611 -0.33665752 -1.8939198 ]]\n",
      "4 1.9446774 [[ 0.60733104  0.73664993 -1.0799555 ]\n",
      " [-0.79007834 -0.03363192  1.7235874 ]\n",
      " [ 0.16691464 -0.3340635  -1.8847224 ]]\n",
      "5 1.9235674 [[ 0.59031737  0.7351025  -1.0613943 ]\n",
      " [-0.77496755 -0.04048849  1.7153332 ]\n",
      " [ 0.15076597 -0.322094   -1.8805432 ]]\n",
      "6 1.9035923 [[ 0.57437915  0.73262864 -1.0429822 ]\n",
      " [-0.75527763 -0.05152001  1.7066748 ]\n",
      " [ 0.1397779  -0.31497923 -1.8766699 ]]\n",
      "7 1.8840802 [[ 0.5581266   0.73071796 -1.024819  ]\n",
      " [-0.7377567  -0.05928206  1.696916  ]\n",
      " [ 0.12698436 -0.30518344 -1.8736721 ]]\n",
      "8 1.8649303 [[ 0.5422525   0.7284691  -1.006696  ]\n",
      " [-0.71909696 -0.06834843  1.6873226 ]\n",
      " [ 0.11586069 -0.29726508 -1.8704668 ]]\n",
      "9 1.8461237 [[ 0.5263195   0.7263868  -0.9886808 ]\n",
      " [-0.7014199  -0.07617545  1.6774726 ]\n",
      " [ 0.10420264 -0.28866172 -1.8674121 ]]\n",
      "10 1.8276567 [[ 0.51057726  0.724167   -0.97071874]\n",
      " [-0.6835445  -0.08428251  1.6677042 ]\n",
      " [ 0.09325797 -0.28090805 -1.8642211 ]]\n",
      "11 1.8095272 [[ 0.49487537  0.72198683 -0.9528367 ]\n",
      " [-0.6662073  -0.09177175  1.6578563 ]\n",
      " [ 0.08226091 -0.27309972 -1.8610324 ]]\n",
      "12 1.7917325 [[ 0.47930723  0.7197362  -0.9350179 ]\n",
      " [-0.64897466 -0.09918405  1.648036  ]\n",
      " [ 0.07167413 -0.26578882 -1.8577565 ]]\n",
      "13 1.7742711 [[ 0.46381763  0.7174805  -0.9172725 ]\n",
      " [-0.6321232  -0.10618488  1.6381854 ]\n",
      " [ 0.06121074 -0.25863767 -1.8544443 ]]\n",
      "14 1.7571387 [[ 0.44844368  0.715177   -0.89959514]\n",
      " [-0.615486   -0.11297941  1.6283427 ]\n",
      " [ 0.05105059 -0.25185692 -1.8510649 ]]\n",
      "15 1.740332 [[ 0.4331642   0.71285117 -0.88198984]\n",
      " [-0.59917414 -0.11943597  1.6184874 ]\n",
      " [ 0.04107857 -0.24531217 -1.8476377 ]]\n",
      "16 1.7238466 [[ 0.4179951   0.7104854  -0.864455  ]\n",
      " [-0.58311963 -0.12563756  1.6086345 ]\n",
      " [ 0.03136769 -0.2390875  -1.8441514 ]]\n",
      "17 1.7076764 [[ 0.40292796  0.70809007 -0.84699255]\n",
      " [-0.5673692  -0.1315301   1.5987766 ]\n",
      " [ 0.02186808 -0.23312514 -1.8406142 ]]\n",
      "18 1.6918167 [[ 0.3879701   0.70565754 -0.8296022 ]\n",
      " [-0.55189335 -0.13714993  1.5889205 ]\n",
      " [ 0.01260981 -0.2274591  -1.837022  ]]\n",
      "19 1.676261 [[ 0.37311807  0.70319235 -0.812285  ]\n",
      " [-0.5367119  -0.1424747   1.5790638 ]\n",
      " [ 0.00356908 -0.22206184 -1.8333784 ]]\n",
      "20 1.6610024 [[ 0.35837552  0.70069116 -0.79504126]\n",
      " [-0.5218103  -0.14752223  1.5692097 ]\n",
      " [-0.00524237 -0.21694621 -1.8296826 ]]\n",
      "21 1.6460342 [[ 0.34374103  0.6981562  -0.7778718 ]\n",
      " [-0.5071965  -0.1522842   1.5593579 ]\n",
      " [-0.01383764 -0.212097   -1.8259366 ]]\n",
      "22 1.6313493 [[ 0.32921636  0.6955861  -0.760777  ]\n",
      " [-0.49286184 -0.15677106  1.5495101 ]\n",
      " [-0.02221331 -0.20751753 -1.8221403 ]]\n",
      "23 1.61694 [[ 0.31480092  0.69298214 -0.7437576 ]\n",
      " [-0.47880825 -0.16098104  1.5396665 ]\n",
      " [-0.03037805 -0.20319764 -1.8182955 ]]\n",
      "24 1.6027992 [[ 0.30049554  0.69034404 -0.7268141 ]\n",
      " [-0.46502945 -0.16492176  1.5298284 ]\n",
      " [-0.03833253 -0.19913614 -1.8144025 ]]\n",
      "25 1.5889187 [[ 0.28629982  0.6876728  -0.7099471 ]\n",
      " [-0.45152405 -0.16859497  1.5199962 ]\n",
      " [-0.04608345 -0.19532502 -1.8104627 ]]\n",
      "26 1.5752912 [[ 0.27221408  0.6849686  -0.69315714]\n",
      " [-0.43828642 -0.17200717  1.5101707 ]\n",
      " [-0.05363356 -0.19176067 -1.806477  ]]\n",
      "27 1.5619091 [[ 0.25823793  0.6822324  -0.67644477]\n",
      " [-0.4253134  -0.175162    1.5003525 ]\n",
      " [-0.06098882 -0.18843597 -1.8024464 ]]\n",
      "28 1.5487646 [[ 0.24437132  0.67946476 -0.65981054]\n",
      " [-0.4125994  -0.17806573  1.4905422 ]\n",
      " [-0.06815303 -0.1853461  -1.798372  ]]\n",
      "29 1.5358508 [[ 0.23061383  0.6766667  -0.64325494]\n",
      " [-0.40014014 -0.1807231   1.4807403 ]\n",
      " [-0.07513179 -0.18248431 -1.794255  ]]\n",
      "30 1.5231597 [[ 0.21696514  0.67383885 -0.6267784 ]\n",
      " [-0.38792992 -0.18314037  1.4709474 ]\n",
      " [-0.08192953 -0.17984515 -1.7900964 ]]\n",
      "31 1.5106845 [[ 0.20342472  0.67098236 -0.61038154]\n",
      " [-0.37596372 -0.18532297  1.4611638 ]\n",
      " [-0.08855163 -0.1774221  -1.7858974 ]]\n",
      "32 1.4984186 [[ 0.1899921   0.66809815 -0.5940647 ]\n",
      " [-0.36423585 -0.18727715  1.4513901 ]\n",
      " [-0.09500287 -0.17520931 -1.7816589 ]]\n",
      "33 1.486355 [[ 0.17666666  0.66518724 -0.57782835]\n",
      " [-0.35274082 -0.18900885  1.4416268 ]\n",
      " [-0.10128841 -0.17320055 -1.7773821 ]]\n",
      "34 1.4744871 [[ 0.16344775  0.66225064 -0.56167287]\n",
      " [-0.3414729  -0.19052424  1.4318743 ]\n",
      " [-0.10741317 -0.17138976 -1.7730682 ]]\n",
      "35 1.4628086 [[ 0.15033466  0.6592895  -0.5455986 ]\n",
      " [-0.3304264  -0.1918294   1.422133  ]\n",
      " [-0.11338217 -0.16977084 -1.7687181 ]]\n",
      "36 1.4513137 [[ 0.13732666  0.65630484 -0.529606  ]\n",
      " [-0.3195955  -0.19293056  1.4124032 ]\n",
      " [-0.11920021 -0.16833785 -1.764333  ]]\n",
      "37 1.4399967 [[ 0.12442294  0.6532978  -0.51369524]\n",
      " [-0.3089745  -0.1938338   1.4026855 ]\n",
      " [-0.12487219 -0.16708483 -1.759914  ]]\n",
      "38 1.428852 [[ 0.11162268  0.65026945 -0.49786666]\n",
      " [-0.29855776 -0.19454527  1.3929802 ]\n",
      " [-0.13040285 -0.16600598 -1.7554622 ]]\n",
      "39 1.4178741 [[ 0.09892502  0.64722097 -0.48212054]\n",
      " [-0.2883396  -0.19507101  1.3832878 ]\n",
      " [-0.13579684 -0.16509557 -1.7509786 ]]\n",
      "40 1.4070581 [[ 0.08632907  0.6441535  -0.4664571 ]\n",
      " [-0.27831447 -0.19541703  1.3736087 ]\n",
      " [-0.1410587  -0.16434796 -1.7464644 ]]\n",
      "41 1.3963993 [[ 0.07383391  0.64106804 -0.45087653]\n",
      " [-0.26847687 -0.19558921  1.3639433 ]\n",
      " [-0.1461929  -0.16375764 -1.7419205 ]]\n",
      "42 1.385893 [[ 0.06143861  0.63796586 -0.43537903]\n",
      " [-0.25882137 -0.19559337  1.354292  ]\n",
      " [-0.15120372 -0.16331922 -1.7373481 ]]\n",
      "43 1.3755349 [[ 0.04914221  0.63484794 -0.4199647 ]\n",
      " [-0.24934274 -0.19543523  1.3446553 ]\n",
      " [-0.15609543 -0.1630274  -1.7327482 ]]\n",
      "44 1.3653208 [[ 0.03694372  0.6317154  -0.4046337 ]\n",
      " [-0.24003579 -0.19512041  1.3350335 ]\n",
      " [-0.1608721  -0.16287711 -1.7281218 ]]\n",
      "45 1.3552469 [[ 0.02484217  0.62856936 -0.3893861 ]\n",
      " [-0.23089546 -0.19465435  1.3254272 ]\n",
      " [-0.1655377  -0.16286325 -1.72347   ]]\n",
      "46 1.3453094 [[ 0.01283654  0.62541085 -0.37422195]\n",
      " [-0.22191682 -0.19404247  1.3158367 ]\n",
      " [-0.17009604 -0.16298102 -1.7187939 ]]\n",
      "47 1.3355048 [[ 9.2582125e-04  6.2224090e-01 -3.5914129e-01]\n",
      " [-2.1309511e-01 -1.9329000e-01  1.3062625e+00]\n",
      " [-1.7455086e-01 -1.6322562e-01 -1.7140944e+00]]\n",
      "48 1.3258293 [[-0.01089101  0.6190606  -0.34414414]\n",
      " [-0.2044257  -0.19240205  1.2967051 ]\n",
      " [-0.17890576 -0.16359243 -1.7093726 ]]\n",
      "49 1.3162804 [[-0.02261496  0.61587083 -0.32923046]\n",
      " [-0.19590394 -0.19138364  1.2871649 ]\n",
      " [-0.1831641  -0.16407698 -1.7046298 ]]\n",
      "50 1.3068547 [[-0.03424709  0.61267275 -0.31440023]\n",
      " [-0.1875256  -0.1902396   1.2776425 ]\n",
      " [-0.18732934 -0.16467486 -1.6998667 ]]\n",
      "51 1.2975491 [[-0.04578841  0.6094672  -0.29965335]\n",
      " [-0.17928633 -0.18897468  1.2681383 ]\n",
      " [-0.1914046  -0.1653819  -1.6950843 ]]\n",
      "52 1.2883614 [[-0.05723998  0.60625523 -0.2849898 ]\n",
      " [-0.17118207 -0.18759346  1.2586528 ]\n",
      " [-0.19539298 -0.1661939  -1.690284  ]]\n",
      "53 1.2792885 [[-0.06860282  0.6030377  -0.27040944]\n",
      " [-0.1632088  -0.18610048  1.2491865 ]\n",
      " [-0.19929741 -0.16710696 -1.6854665 ]]\n",
      "54 1.2703284 [[-0.07987797  0.59981555 -0.25591213]\n",
      " [-0.15536271 -0.18450002  1.23974   ]\n",
      " [-0.20312078 -0.1681171  -1.680633  ]]\n",
      "55 1.2614783 [[-0.09106645  0.5965896  -0.24149771]\n",
      " [-0.14764005 -0.1827963   1.2303137 ]\n",
      " [-0.20686577 -0.1692206  -1.6757845 ]]\n",
      "56 1.2527364 [[-0.10216931  0.5933608  -0.22716603]\n",
      " [-0.14003725 -0.18099348  1.220908  ]\n",
      " [-0.210535   -0.17041385 -1.670922  ]]\n",
      "57 1.2441002 [[-0.11318757  0.5901299  -0.2129169 ]\n",
      " [-0.1325509  -0.17909549  1.2115237 ]\n",
      " [-0.21413101 -0.17169325 -1.6660466 ]]\n",
      "58 1.2355676 [[-0.12412225  0.5868978  -0.19875011]\n",
      " [-0.1251776  -0.17710626  1.2021611 ]\n",
      " [-0.21765615 -0.17305541 -1.6611593 ]]\n",
      "59 1.2271373 [[-0.13497436  0.58366525 -0.18466544]\n",
      " [-0.11791418 -0.17502943  1.1928208 ]\n",
      " [-0.22111271 -0.17449693 -1.6562612 ]]\n",
      "60 1.218807 [[-0.14574492  0.5804331  -0.17066267]\n",
      " [-0.11075749 -0.17286873  1.1835034 ]\n",
      " [-0.22450286 -0.17601466 -1.6513534 ]]\n",
      "61 1.2105749 [[-0.15643494  0.57720196 -0.15674154]\n",
      " [-0.1037046  -0.17062765  1.1742095 ]\n",
      " [-0.22782873 -0.17760542 -1.6464367 ]]\n",
      "62 1.2024395 [[-0.16704541  0.5739727  -0.1429018 ]\n",
      " [-0.09675263 -0.16830958  1.1649394 ]\n",
      " [-0.23109227 -0.17926611 -1.6415124 ]]\n",
      "63 1.1943992 [[-0.17757732  0.57074594 -0.12914315]\n",
      " [-0.08989877 -0.16591793  1.1556939 ]\n",
      " [-0.23429538 -0.18099388 -1.6365815 ]]\n",
      "64 1.1864524 [[-0.18803164  0.5675224  -0.11546531]\n",
      " [-0.08314046 -0.16345584  1.1464735 ]\n",
      " [-0.23743993 -0.18278578 -1.6316451 ]]\n",
      "65 1.1785977 [[-0.19840933  0.5643028  -0.101868  ]\n",
      " [-0.07647501 -0.16092645  1.1372787 ]\n",
      " [-0.24052756 -0.18463904 -1.6267042 ]]\n",
      "66 1.1708337 [[-0.20871139  0.5610877  -0.08835089]\n",
      " [-0.0699001  -0.1583328   1.1281102 ]\n",
      " [-0.24356002 -0.18655095 -1.6217599 ]]\n",
      "67 1.1631591 [[-0.21893872  0.55787784 -0.07491367]\n",
      " [-0.06341324 -0.15567784  1.1189684 ]\n",
      " [-0.24653876 -0.1885189  -1.6168132 ]]\n",
      "68 1.1555727 [[-0.22909231  0.5546738  -0.06155602]\n",
      " [-0.05701226 -0.15296437  1.1098539 ]\n",
      " [-0.24946538 -0.19054027 -1.6118652 ]]\n",
      "69 1.148073 [[-0.23917307  0.5514761  -0.04827759]\n",
      " [-0.05069489 -0.15019524  1.1007674 ]\n",
      " [-0.2523412  -0.19261266 -1.6069169 ]]\n",
      "70 1.1406591 [[-0.24918191  0.5482854  -0.03507805]\n",
      " [-0.04445908 -0.14737304  1.0917094 ]\n",
      " [-0.25516763 -0.19473356 -1.6019696 ]]\n",
      "71 1.1333299 [[-0.25911975  0.54510224 -0.02195705]\n",
      " [-0.03830277 -0.1445005   1.0826805 ]\n",
      " [-0.2579459  -0.19690073 -1.5970242 ]]\n",
      "72 1.1260841 [[-0.26898748  0.54192716 -0.00891422]\n",
      " [-0.0322241  -0.14158     1.0736814 ]\n",
      " [-0.26067728 -0.19911174 -1.5920818 ]]\n",
      "73 1.1189209 [[-0.278786    0.53876066  0.00405078]\n",
      " [-0.02622108 -0.1386141   1.0647124 ]\n",
      " [-0.26336282 -0.20136447 -1.5871434 ]]\n",
      "74 1.111839 [[-0.2885162   0.5356033   0.01693832]\n",
      " [-0.02029205 -0.13560516  1.0557745 ]\n",
      " [-0.26600376 -0.20365672 -1.5822102 ]]\n",
      "75 1.1048378 [[-0.29817888  0.53245556  0.02974876]\n",
      " [-0.01443519 -0.13255551  1.046868  ]\n",
      " [-0.268601   -0.20598638 -1.5772833 ]]\n",
      "76 1.0979162 [[-0.30777496  0.5293179   0.04248247]\n",
      " [-0.00864889 -0.12946735  1.0379936 ]\n",
      " [-0.27115557 -0.20835134 -1.5723637 ]]\n",
      "77 1.0910733 [[-0.31730524  0.5261908   0.05513985]\n",
      " [-0.00293157 -0.12634298  1.0291519 ]\n",
      " [-0.27366838 -0.21074972 -1.5674525 ]]\n",
      "78 1.0843083 [[-0.32677054  0.52307475  0.06772124]\n",
      " [ 0.00271828 -0.12318441  1.0203435 ]\n",
      " [-0.27614033 -0.21317942 -1.5625509 ]]\n",
      "79 1.0776203 [[-0.33617172  0.5199701   0.08022705]\n",
      " [ 0.00830217 -0.11999381  1.011569  ]\n",
      " [-0.2785722  -0.21563867 -1.5576597 ]]\n",
      "80 1.0710084 [[-0.34550956  0.51687735  0.09265764]\n",
      " [ 0.0138214  -0.11677311  1.0028291 ]\n",
      " [-0.28096488 -0.21812549 -1.5527803 ]]\n",
      "81 1.0644723 [[-0.35478482  0.51379687  0.1050134 ]\n",
      " [ 0.01927745 -0.11352438  0.9941243 ]\n",
      " [-0.28331897 -0.22063822 -1.5479134 ]]\n",
      "82 1.0580108 [[-0.36399832  0.5107291   0.11729471]\n",
      " [ 0.02467152 -0.11024939  0.9854553 ]\n",
      " [-0.28563523 -0.2231749  -1.5430604 ]]\n",
      "83 1.0516233 [[-0.3731508   0.50767434  0.12950194]\n",
      " [ 0.0300049  -0.1069501   0.9768226 ]\n",
      " [-0.28791428 -0.22573395 -1.5382223 ]]\n",
      "84 1.0453093 [[-0.38224304  0.504633    0.14163549]\n",
      " [ 0.03527871 -0.10362826  0.96822697]\n",
      " [-0.29015678 -0.22831362 -1.5334002 ]]\n",
      "85 1.039068 [[-0.39127576  0.5016055   0.15369573]\n",
      " [ 0.04049413 -0.10028567  0.95966893]\n",
      " [-0.2923633  -0.2309123  -1.528595  ]]\n",
      "86 1.0328984 [[-0.4002497   0.49859214  0.16568305]\n",
      " [ 0.04565228 -0.09692406  0.95114917]\n",
      " [-0.29453433 -0.2335284  -1.5238079 ]]\n",
      "87 1.0268005 [[-0.40916556  0.49559325  0.1775978 ]\n",
      " [ 0.05075414 -0.093545    0.94266826]\n",
      " [-0.29667044 -0.23616028 -1.5190399 ]]\n",
      "88 1.0207732 [[-0.41802406  0.49260917  0.1894404 ]\n",
      " [ 0.0558008  -0.09015026  0.9342269 ]\n",
      " [-0.29877204 -0.2388065  -1.514292  ]]\n",
      "89 1.0148159 [[-0.4268259   0.48964024  0.20121118]\n",
      " [ 0.06079311 -0.08674131  0.9258256 ]\n",
      " [-0.3008397  -0.24146548 -1.5095654 ]]\n",
      "90 1.0089284 [[-0.43557176  0.48668674  0.21291055]\n",
      " [ 0.06573213 -0.08331978  0.91746503]\n",
      " [-0.30287364 -0.24413583 -1.504861  ]]\n",
      "91 1.0031098 [[-0.44426233  0.48374897  0.22453886]\n",
      " [ 0.07061861 -0.07988708  0.90914583]\n",
      " [-0.30487445 -0.24681602 -1.50018   ]]\n",
      "92 0.9973599 [[-0.4528982   0.4808272   0.2360965 ]\n",
      " [ 0.0754535  -0.0764448   0.90086865]\n",
      " [-0.30684236 -0.24950479 -1.4955233 ]]\n",
      "93 0.9916775 [[-0.46148008  0.47792178  0.24758382]\n",
      " [ 0.08023753 -0.07299429  0.8926341 ]\n",
      " [-0.30877778 -0.25220066 -1.490892  ]]\n",
      "94 0.9860627 [[-0.47000858  0.47503293  0.2590012 ]\n",
      " [ 0.08497152 -0.06953701  0.8844428 ]\n",
      " [-0.310681   -0.2549024  -1.4862871 ]]\n",
      "95 0.98051476 [[-0.47848433  0.47216088  0.270349  ]\n",
      " [ 0.0896562  -0.06607425  0.8762954 ]\n",
      " [-0.31255227 -0.2576086  -1.4817096 ]]\n",
      "96 0.9750331 [[-0.48690796  0.46930593  0.28162757]\n",
      " [ 0.0942923  -0.06260743  0.8681925 ]\n",
      " [-0.3143919  -0.26031807 -1.4771605 ]]\n",
      "97 0.96961737 [[-0.49528003  0.4664683   0.29283726]\n",
      " [ 0.09888047 -0.05913777  0.86013466]\n",
      " [-0.31620014 -0.2630295  -1.4726408 ]]\n",
      "98 0.9642669 [[-0.50360113  0.46364823  0.30397847]\n",
      " [ 0.10342138 -0.05566658  0.8521226 ]\n",
      " [-0.3179772  -0.2657417  -1.4681515 ]]\n",
      "99 0.9589814 [[-0.5118719   0.46084595  0.31505153]\n",
      " [ 0.10791562 -0.05219511  0.84415686]\n",
      " [-0.3197233  -0.2684535  -1.4636935 ]]\n",
      "100 0.9537603 [[-0.52009284  0.45806167  0.32605678]\n",
      " [ 0.11236385 -0.04872456  0.8362381 ]\n",
      " [-0.32143867 -0.27116376 -1.4592679 ]]\n",
      "101 0.94860303 [[-0.5282646   0.45529562  0.33699456]\n",
      " [ 0.11676657 -0.04525609  0.8283669 ]\n",
      " [-0.32312346 -0.27387127 -1.4548756 ]]\n",
      "102 0.9435093 [[-0.5363876   0.452548    0.34786522]\n",
      " [ 0.12112439 -0.04179091  0.8205439 ]\n",
      " [-0.32477778 -0.27657503 -1.4505175 ]]\n",
      "103 0.9384786 [[-0.5444625   0.449819    0.35866913]\n",
      " [ 0.1254377  -0.03833003  0.8127697 ]\n",
      " [-0.32640195 -0.2792738  -1.4461945 ]]\n",
      "104 0.93351036 [[-0.55248976  0.4471088   0.3694066 ]\n",
      " [ 0.12970722 -0.03487467  0.8050449 ]\n",
      " [-0.32799587 -0.28196672 -1.4419076 ]]\n",
      "105 0.9286042 [[-0.5604699   0.4444176   0.380078  ]\n",
      " [ 0.13393322 -0.03142577  0.79737   ]\n",
      " [-0.32955986 -0.28465256 -1.4376578 ]]\n",
      "106 0.9237596 [[-0.5684035   0.44174555  0.39068362]\n",
      " [ 0.13811627 -0.02798455  0.78974575]\n",
      " [-0.3310939  -0.2873305  -1.4334458 ]]\n",
      "107 0.91897637 [[-0.57629097  0.43909284  0.40122384]\n",
      " [ 0.14225672 -0.02455183  0.7821726 ]\n",
      " [-0.33259824 -0.28999943 -1.4292725 ]]\n",
      "108 0.9142538 [[-0.58413285  0.4364596   0.41169894]\n",
      " [ 0.14635509 -0.02112876  0.77465117]\n",
      " [-0.3340728  -0.29265848 -1.425139  ]]\n",
      "109 0.90959144 [[-0.5919296   0.43384603  0.4221093 ]\n",
      " [ 0.15041167 -0.01771621  0.76718205]\n",
      " [-0.33551782 -0.29530665 -1.4210458 ]]\n",
      "110 0.904989 [[-0.59968174  0.4312522   0.4324552 ]\n",
      " [ 0.15442692 -0.01431517  0.75976574]\n",
      " [-0.33693328 -0.29794312 -1.4169939 ]]\n",
      "111 0.9004459 [[-0.6073897   0.42867833  0.442737  ]\n",
      " [ 0.15840118 -0.01092653  0.75240284]\n",
      " [-0.33831927 -0.3005669  -1.412984  ]]\n",
      "112 0.8959619 [[-0.6150539   0.42612454  0.45295504]\n",
      " [ 0.16233478 -0.00755119  0.7450939 ]\n",
      " [-0.33967587 -0.30317724 -1.4090171 ]]\n",
      "113 0.89153624 [[-0.6226748   0.42359093  0.4631096 ]\n",
      " [ 0.16622809 -0.00419004  0.73783946]\n",
      " [-0.34100312 -0.30577326 -1.4050938 ]]\n",
      "114 0.88716865 [[-6.3025296e-01  4.2107764e-01  4.7320104e-01]\n",
      " [ 1.7008136e-01 -8.4386580e-04  7.3063999e-01]\n",
      " [-3.4230110e-01 -3.0835411e-01 -1.4012150e+00]]\n",
      "115 0.88285875 [[-0.6377887   0.41858476  0.48322964]\n",
      " [ 0.173895    0.00248641  0.7234961 ]\n",
      " [-0.34356982 -0.31091914 -1.3973812 ]]\n",
      "116 0.87860584 [[-0.6452825   0.41611242  0.49319574]\n",
      " [ 0.17766921  0.00580011  0.71640813]\n",
      " [-0.34480935 -0.31346744 -1.3935933 ]]\n",
      "117 0.8744097 [[-0.6527347   0.4136607   0.5030997 ]\n",
      " [ 0.18140434  0.00909634  0.70937675]\n",
      " [-0.3460197  -0.31599844 -1.3898519 ]]\n",
      "118 0.8702697 [[-0.6601458   0.41122973  0.5129418 ]\n",
      " [ 0.18510057  0.01237451  0.70240235]\n",
      " [-0.347201   -0.31851122 -1.3861579 ]]\n",
      "119 0.86618567 [[-0.6675162   0.40881956  0.5227223 ]\n",
      " [ 0.18875834  0.01563369  0.6954854 ]\n",
      " [-0.3483531  -0.3210053  -1.3825117 ]]\n",
      "120 0.8621566 [[-0.67484623  0.4064303   0.5324416 ]\n",
      " [ 0.19237764  0.01887336  0.68862647]\n",
      " [-0.34947628 -0.3234799  -1.378914  ]]\n",
      "121 0.85818255 [[-0.68213636  0.404062    0.5421    ]\n",
      " [ 0.19595896  0.02209269  0.6818258 ]\n",
      " [-0.35057032 -0.32593444 -1.3753654 ]]\n",
      "122 0.8542628 [[-0.6893869   0.40171477  0.55169785]\n",
      " [ 0.19950235  0.02529116  0.67508394]\n",
      " [-0.35163546 -0.32836825 -1.3718665 ]]\n",
      "123 0.85039675 [[-0.6965983   0.3993886   0.5612354 ]\n",
      " [ 0.20300815  0.028468    0.6684013 ]\n",
      " [-0.35267162 -0.33078083 -1.3684177 ]]\n",
      "124 0.8465842 [[-0.70377094  0.39708364  0.57071304]\n",
      " [ 0.20647654  0.0316227   0.6617782 ]\n",
      " [-0.35367888 -0.33317152 -1.3650198 ]]\n",
      "125 0.84282446 [[-0.71090513  0.3947999   0.580131  ]\n",
      " [ 0.20990773  0.03475461  0.65521514]\n",
      " [-0.3546573  -0.33553985 -1.3616731 ]]\n",
      "126 0.8391171 [[-0.7180013   0.3925374   0.58948964]\n",
      " [ 0.21330196  0.03786319  0.64871234]\n",
      " [-0.35560685 -0.3378853  -1.3583782 ]]\n",
      "127 0.8354615 [[-0.7250598   0.3902962   0.59878933]\n",
      " [ 0.21665937  0.0409479   0.6422702 ]\n",
      " [-0.35652763 -0.34020734 -1.3551353 ]]\n",
      "128 0.83185726 [[-0.73208094  0.38807634  0.6080303 ]\n",
      " [ 0.21998021  0.04400819  0.63588905]\n",
      " [-0.35741964 -0.34250554 -1.351945  ]]\n",
      "129 0.8283038 [[-0.7390651   0.38587785  0.61721295]\n",
      " [ 0.22326463  0.04704362  0.62956923]\n",
      " [-0.35828298 -0.34477943 -1.3488078 ]]\n",
      "130 0.8248006 [[-0.7460126   0.38370073  0.6263376 ]\n",
      " [ 0.2265129   0.05005361  0.623311  ]\n",
      " [-0.35911763 -0.34702867 -1.3457239 ]]\n",
      "131 0.8213473 [[-0.75292385  0.381545    0.6354045 ]\n",
      " [ 0.2297251   0.05303786  0.61711454]\n",
      " [-0.35992375 -0.34925273 -1.3426937 ]]\n",
      "132 0.81794316 [[-0.7597991   0.37941068  0.6444141 ]\n",
      " [ 0.2329015   0.05599577  0.6109802 ]\n",
      " [-0.3607013  -0.3514514  -1.3397175 ]]\n",
      "133 0.8145876 [[-0.7666388   0.3772978   0.6533667 ]\n",
      " [ 0.2360422   0.05892705  0.6049082 ]\n",
      " [-0.3614504  -0.35362422 -1.3367956 ]]\n",
      "134 0.81128025 [[-0.77344316  0.3752063   0.66226256]\n",
      " [ 0.23914747  0.06183125  0.59889877]\n",
      " [-0.36217108 -0.35577095 -1.3339281 ]]\n",
      "135 0.8080205 [[-0.7802126   0.37313622  0.67110205]\n",
      " [ 0.2422174   0.06470806  0.592952  ]\n",
      " [-0.36286348 -0.3578912  -1.3311155 ]]\n",
      "136 0.80480766 [[-0.78694737  0.37108752  0.6798855 ]\n",
      " [ 0.24525225  0.06755707  0.58706814]\n",
      " [-0.36352763 -0.35998482 -1.3283577 ]]\n",
      "137 0.80164146 [[-0.7936478   0.3690602   0.6886133 ]\n",
      " [ 0.24825215  0.07037804  0.58124727]\n",
      " [-0.36416364 -0.36205146 -1.3256551 ]]\n",
      "138 0.79852104 [[-0.8003143   0.36705422  0.6972857 ]\n",
      " [ 0.2512173   0.0731706   0.5754896 ]\n",
      " [-0.36477157 -0.36409095 -1.3230077 ]]\n",
      "139 0.795446 [[-0.80694705  0.36506957  0.7059032 ]\n",
      " [ 0.25414786  0.07593453  0.56979513]\n",
      " [-0.36535153 -0.36610305 -1.3204156 ]]\n",
      "140 0.79241574 [[-0.8135464   0.3631062   0.7144659 ]\n",
      " [ 0.25704402  0.07866956  0.5641639 ]\n",
      " [-0.36590365 -0.3680876  -1.317879  ]]\n",
      "141 0.7894295 [[-0.82011276  0.36116406  0.72297436]\n",
      " [ 0.25990596  0.08137543  0.55859613]\n",
      " [-0.366428   -0.37004447 -1.3153977 ]]\n",
      "142 0.7864869 [[-0.8266463   0.35924312  0.73142886]\n",
      " [ 0.26273385  0.08405197  0.5530917 ]\n",
      " [-0.36692473 -0.3719735  -1.312972  ]]\n",
      "143 0.7835874 [[-0.8331474   0.35734335  0.7398298 ]\n",
      " [ 0.26552787  0.086699    0.54765064]\n",
      " [-0.36739394 -0.37387457 -1.3106017 ]]\n",
      "144 0.78073025 [[-0.83961636  0.35546464  0.7481774 ]\n",
      " [ 0.26828822  0.08931631  0.542273  ]\n",
      " [-0.3678358  -0.37574762 -1.3082868 ]]\n",
      "145 0.77791494 [[-0.8460534   0.35360697  0.7564722 ]\n",
      " [ 0.2710151   0.09190376  0.53695863]\n",
      " [-0.36825037 -0.37759256 -1.3060273 ]]\n",
      "146 0.7751409 [[-0.8524589   0.35177025  0.76471436]\n",
      " [ 0.27370867  0.09446124  0.5317076 ]\n",
      " [-0.3686379  -0.37940934 -1.303823  ]]\n",
      "147 0.7724074 [[-0.85883313  0.34995443  0.7729044 ]\n",
      " [ 0.27636915  0.09698863  0.5265197 ]\n",
      " [-0.36899844 -0.38119796 -1.3016738 ]]\n",
      "148 0.769714 [[-0.8651764   0.34815943  0.78104264]\n",
      " [ 0.2789967   0.09948589  0.5213949 ]\n",
      " [-0.36933222 -0.38295835 -1.2995796 ]]\n",
      "149 0.76706 [[-0.8714889   0.34638515  0.78912944]\n",
      " [ 0.28159156  0.10195285  0.5163331 ]\n",
      " [-0.36963937 -0.38469064 -1.2975402 ]]\n",
      "150 0.76444477 [[-0.877771    0.34463155  0.79716516]\n",
      " [ 0.28415388  0.10438961  0.51133406]\n",
      " [-0.36992007 -0.3863947  -1.2955554 ]]\n",
      "151 0.7618679 [[-0.884023    0.34289852  0.80515015]\n",
      " [ 0.28668392  0.10679599  0.5063976 ]\n",
      " [-0.37017447 -0.38807076 -1.2936249 ]]\n",
      "152 0.7593286 [[-0.89024514  0.34118596  0.81308484]\n",
      " [ 0.28918183  0.1091721   0.5015236 ]\n",
      " [-0.3704028  -0.3897187  -1.2917485 ]]\n",
      "153 0.7568263 [[-0.8964377   0.33949378  0.8209696 ]\n",
      " [ 0.29164788  0.11151786  0.49671182]\n",
      " [-0.3706052  -0.3913388  -1.289926  ]]\n",
      "154 0.75436056 [[-0.90260094  0.33782187  0.82880473]\n",
      " [ 0.29408222  0.11383336  0.49196202]\n",
      " [-0.3707819  -0.392931   -1.2881571 ]]\n",
      "155 0.7519305 [[-0.9087352   0.33617014  0.8365907 ]\n",
      " [ 0.2964851   0.1161186   0.48727393]\n",
      " [-0.37093312 -0.39449555 -1.2864413 ]]\n",
      "156 0.7495358 [[-0.9148407   0.3345385   0.84432787]\n",
      " [ 0.29885674  0.11837362  0.48264727]\n",
      " [-0.371059   -0.3960325  -1.2847785 ]]\n",
      "157 0.7471759 [[-0.92091775  0.3329268   0.85201657]\n",
      " [ 0.30119735  0.12059855  0.47808176]\n",
      " [-0.37115982 -0.39754206 -1.2831681 ]]\n",
      "158 0.7448497 [[-0.92696655  0.33133498  0.8596572 ]\n",
      " [ 0.30350715  0.12279347  0.47357705]\n",
      " [-0.37123576 -0.39902437 -1.2816098 ]]\n",
      "159 0.7425573 [[-0.93298745  0.32976288  0.8672502 ]\n",
      " [ 0.30578637  0.12495846  0.4691328 ]\n",
      " [-0.37128705 -0.4004796  -1.2801032 ]]\n",
      "160 0.7402977 [[-0.93898064  0.32821035  0.8747959 ]\n",
      " [ 0.30803528  0.12709364  0.4647487 ]\n",
      " [-0.3713139  -0.401908   -1.2786479 ]]\n",
      "161 0.73807037 [[-0.94494647  0.32667735  0.8822948 ]\n",
      " [ 0.31025407  0.1291992   0.46042436]\n",
      " [-0.3713166  -0.4033097  -1.2772435 ]]\n",
      "162 0.7358748 [[-0.9508851   0.3251637   0.8897471 ]\n",
      " [ 0.31244305  0.1312752   0.45615938]\n",
      " [-0.37129533 -0.40468505 -1.2758894 ]]\n",
      "163 0.7337104 [[-0.95679694  0.32366928  0.8971533 ]\n",
      " [ 0.31460232  0.13332197  0.45195335]\n",
      " [-0.37125042 -0.40603408 -1.2745852 ]]\n",
      "164 0.7315768 [[-0.9626821   0.32219395  0.90451384]\n",
      " [ 0.31673235  0.13533945  0.44780585]\n",
      " [-0.37118194 -0.40735725 -1.2733306 ]]\n",
      "165 0.729473 [[-0.96854097  0.3207376   0.911829  ]\n",
      " [ 0.31883314  0.13732803  0.4437165 ]\n",
      " [-0.37109035 -0.40865463 -1.2721248 ]]\n",
      "166 0.72739875 [[-0.97437364  0.3193001   0.9190992 ]\n",
      " [ 0.32090512  0.13928775  0.43968478]\n",
      " [-0.37097576 -0.40992665 -1.2709674 ]]\n",
      "167 0.7253535 [[-0.9801805   0.31788126  0.9263249 ]\n",
      " [ 0.32294846  0.14121896  0.43571025]\n",
      " [-0.37083852 -0.41117346 -1.2698578 ]]\n",
      "168 0.7233366 [[-0.98596174  0.316481    0.9335064 ]\n",
      " [ 0.32496345  0.1431218   0.4317924 ]\n",
      " [-0.37067884 -0.41239545 -1.2687955 ]]\n",
      "169 0.7213476 [[-0.99171764  0.31509915  0.94064415]\n",
      " [ 0.32695037  0.1449965   0.4279308 ]\n",
      " [-0.37049696 -0.41359285 -1.26778   ]]\n",
      "170 0.71938586 [[-0.99744844  0.31373557  0.9477385 ]\n",
      " [ 0.32890943  0.14684336  0.4241249 ]\n",
      " [-0.3702932  -0.41476592 -1.2668107 ]]\n",
      "171 0.717451 [[-1.0031544   0.31239012  0.9547899 ]\n",
      " [ 0.33084095  0.14866255  0.42037418]\n",
      " [-0.37006778 -0.41591507 -1.2658869 ]]\n",
      "172 0.7155423 [[-1.0088357   0.31106266  0.96179867]\n",
      " [ 0.3327451   0.15045442  0.41667813]\n",
      " [-0.36982104 -0.41704053 -1.2650082 ]]\n",
      "173 0.7136595 [[-1.0144926   0.30975303  0.96876526]\n",
      " [ 0.3346223   0.15221913  0.41303623]\n",
      " [-0.36955315 -0.4181427  -1.2641739 ]]\n",
      "174 0.71180195 [[-1.0201255   0.3084611   0.97569   ]\n",
      " [ 0.33647266  0.1539571   0.4094479 ]\n",
      " [-0.3692645  -0.4192218  -1.2633834 ]]\n",
      "175 0.7099691 [[-1.0257344   0.3071867   0.98257333]\n",
      " [ 0.3382966   0.15566845  0.40591264]\n",
      " [-0.36895528 -0.42027825 -1.2626362 ]]\n",
      "176 0.70816064 [[-1.0313197   0.3059297   0.9894156 ]\n",
      " [ 0.34009427  0.15735355  0.40242985]\n",
      " [-0.36862582 -0.4213124  -1.2619315 ]]\n",
      "177 0.7063759 [[-1.0368816   0.30468994  0.9962172 ]\n",
      " [ 0.341866    0.1590127   0.39899895]\n",
      " [-0.3682764  -0.42232448 -1.2612689 ]]\n",
      "178 0.7046145 [[-1.0424203   0.30346727  1.0029786 ]\n",
      " [ 0.34361207  0.16064617  0.3956194 ]\n",
      " [-0.36790723 -0.42331493 -1.2606475 ]]\n",
      "179 0.70287603 [[-1.047936    0.30226156  1.0097001 ]\n",
      " [ 0.34533274  0.1622543   0.3922906 ]\n",
      " [-0.36751866 -0.424284   -1.260067  ]]\n",
      "180 0.70115983 [[-1.053429    0.30107263  1.016382  ]\n",
      " [ 0.34702834  0.16383733  0.38901198]\n",
      " [-0.36711094 -0.42523217 -1.2595265 ]]\n",
      "181 0.6994658 [[-1.0588995   0.29990032  1.0230248 ]\n",
      " [ 0.3486991   0.16539568  0.38578293]\n",
      " [-0.36668438 -0.42615965 -1.2590256 ]]\n",
      "182 0.6977931 [[-1.0643477   0.2987445   1.0296289 ]\n",
      " [ 0.3503453   0.16692953  0.38260287]\n",
      " [-0.36623922 -0.42706692 -1.2585634 ]]\n",
      "183 0.6961416 [[-1.0697739   0.297605    1.0361946 ]\n",
      " [ 0.35196722  0.16843931  0.37947118]\n",
      " [-0.3657758  -0.42795426 -1.2581395 ]]\n",
      "184 0.6945107 [[-1.0751783   0.2964817   1.0427222 ]\n",
      " [ 0.3535652   0.16992529  0.37638727]\n",
      " [-0.36529434 -0.42882204 -1.2577531 ]]\n",
      "185 0.69290006 [[-1.080561    0.2953744   1.0492123 ]\n",
      " [ 0.3551394   0.1713878   0.37335053]\n",
      " [-0.36479515 -0.42967057 -1.2574037 ]]\n",
      "186 0.6913092 [[-1.0859225   0.29428297  1.0556651 ]\n",
      " [ 0.35669023  0.17282715  0.37036034]\n",
      " [-0.36427847 -0.4305003  -1.2570907 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 0.6897378 [[-1.0912627   0.29320726  1.0620811 ]\n",
      " [ 0.35821787  0.17424372  0.36741614]\n",
      " [-0.36374465 -0.4313115  -1.2568133 ]]\n",
      "188 0.68818545 [[-1.0965819   0.2921471   1.0684606 ]\n",
      " [ 0.35972267  0.17563777  0.36451727]\n",
      " [-0.3631939  -0.43210456 -1.2565709 ]]\n",
      "189 0.68665165 [[-1.1018804   0.29110235  1.0748038 ]\n",
      " [ 0.36120486  0.17700969  0.36166313]\n",
      " [-0.36262655 -0.4328798  -1.256363  ]]\n",
      "190 0.68513626 [[-1.1071584   0.29007286  1.0811113 ]\n",
      " [ 0.3626648   0.17835976  0.35885313]\n",
      " [-0.3620428  -0.43363762 -1.2561889 ]]\n",
      "191 0.68363875 [[-1.1124161   0.28905848  1.0873834 ]\n",
      " [ 0.36410266  0.17968836  0.35608667]\n",
      " [-0.36144304 -0.43437833 -1.256048  ]]\n",
      "192 0.6821586 [[-1.1176537   0.28805906  1.0936204 ]\n",
      " [ 0.3655188   0.18099575  0.35336313]\n",
      " [-0.36082742 -0.43510234 -1.2559396 ]]\n",
      "193 0.6806959 [[-1.1228714   0.28707445  1.0998226 ]\n",
      " [ 0.36691344  0.18228236  0.35068187]\n",
      " [-0.3601963  -0.43580988 -1.2558632 ]]\n",
      "194 0.67924976 [[-1.1280694   0.28610447  1.1059905 ]\n",
      " [ 0.36828694  0.1835484   0.34804234]\n",
      " [-0.35954988 -0.4365014  -1.2558181 ]]\n",
      "195 0.6778203 [[-1.1332479   0.285149    1.1121244 ]\n",
      " [ 0.3696395   0.18479432  0.3454439 ]\n",
      " [-0.35888848 -0.43717718 -1.2558037 ]]\n",
      "196 0.676407 [[-1.138407    0.2842079   1.1182246 ]\n",
      " [ 0.37097144  0.18602036  0.34288594]\n",
      " [-0.35821232 -0.4378376  -1.2558194 ]]\n",
      "197 0.67500937 [[-1.143547    0.28328103  1.1242915 ]\n",
      " [ 0.37228298  0.18722689  0.34036785]\n",
      " [-0.3575217  -0.43848294 -1.2558647 ]]\n",
      "198 0.6736274 [[-1.1486682   0.2823682   1.1303254 ]\n",
      " [ 0.37357447  0.18841419  0.33788908]\n",
      " [-0.35681683 -0.4391136  -1.255939  ]]\n",
      "199 0.67226064 [[-1.1537706   0.28146932  1.1363267 ]\n",
      " [ 0.37484607  0.18958263  0.335449  ]\n",
      " [-0.35609803 -0.4397298  -1.2560415 ]]\n",
      "200 0.6709087 [[-1.1588544   0.2805842   1.1422956 ]\n",
      " [ 0.37609816  0.1907325   0.33304706]\n",
      " [-0.3553655  -0.44033197 -1.256172  ]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non_normalized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  516805400000.0 \n",
      "Prediction:\n",
      " [[ 508047.06]\n",
      " [1021539.2 ]\n",
      " [ 803854.94]\n",
      " [ 563844.2 ]\n",
      " [ 664315.3 ]\n",
      " [ 669895.  ]\n",
      " [ 614069.8 ]\n",
      " [ 781512.94]]\n",
      "1 Cost:  5.678039e+26 \n",
      "Prediction:\n",
      " [[-1.6808560e+13]\n",
      " [-3.3837355e+13]\n",
      " [-2.6618626e+13]\n",
      " [-1.8659515e+13]\n",
      " [-2.1991235e+13]\n",
      " [-2.2176332e+13]\n",
      " [-2.0325375e+13]\n",
      " [-2.5878243e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[5.57142386e+20]\n",
      " [1.12158475e+21]\n",
      " [8.82310266e+20]\n",
      " [6.18494818e+20]\n",
      " [7.28929203e+20]\n",
      " [7.35064443e+20]\n",
      " [6.73711975e+20]\n",
      " [8.57769307e+20]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[-1.8467237e+28]\n",
      " [-3.7176441e+28]\n",
      " [-2.9245365e+28]\n",
      " [-2.0500846e+28]\n",
      " [-2.4161341e+28]\n",
      " [-2.4364703e+28]\n",
      " [-2.2331094e+28]\n",
      " [-2.8431921e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[6.1212154e+35]\n",
      " [1.2322634e+36]\n",
      " [9.6937717e+35]\n",
      " [6.7952822e+35]\n",
      " [8.0086037e+35]\n",
      " [8.0760102e+35]\n",
      " [7.4019426e+35]\n",
      " [9.4241448e+35]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min-max scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  0.170495 \n",
      "Prediction:\n",
      " [[0.6861256 ]\n",
      " [1.7213063 ]\n",
      " [1.1293144 ]\n",
      " [0.38493544]\n",
      " [0.8108075 ]\n",
      " [0.63281506]\n",
      " [0.35789388]\n",
      " [0.2547381 ]]\n",
      "1 Cost:  0.17049003 \n",
      "Prediction:\n",
      " [[0.68611443]\n",
      " [1.7212934 ]\n",
      " [1.1293042 ]\n",
      " [0.384928  ]\n",
      " [0.8107984 ]\n",
      " [0.6328061 ]\n",
      " [0.3578875 ]\n",
      " [0.25473118]]\n",
      "2 Cost:  0.17048508 \n",
      "Prediction:\n",
      " [[0.6861033 ]\n",
      " [1.7212808 ]\n",
      " [1.1292937 ]\n",
      " [0.38492036]\n",
      " [0.81078917]\n",
      " [0.6327972 ]\n",
      " [0.35788125]\n",
      " [0.2547242 ]]\n",
      "3 Cost:  0.17047995 \n",
      "Prediction:\n",
      " [[0.68609226]\n",
      " [1.7212673 ]\n",
      " [1.1292833 ]\n",
      " [0.3849128 ]\n",
      " [0.81078017]\n",
      " [0.63278824]\n",
      " [0.35787487]\n",
      " [0.2547173 ]]\n",
      "4 Cost:  0.17047492 \n",
      "Prediction:\n",
      " [[0.68608123]\n",
      " [1.7212543 ]\n",
      " [1.1292729 ]\n",
      " [0.38490516]\n",
      " [0.8107709 ]\n",
      " [0.6327793 ]\n",
      " [0.3578685 ]\n",
      " [0.25471032]]\n",
      "5 Cost:  0.17046991 \n",
      "Prediction:\n",
      " [[0.6860701 ]\n",
      " [1.7212415 ]\n",
      " [1.1292624 ]\n",
      " [0.3848976 ]\n",
      " [0.8107617 ]\n",
      " [0.6327703 ]\n",
      " [0.35786223]\n",
      " [0.2547034 ]]\n",
      "6 Cost:  0.1704649 \n",
      "Prediction:\n",
      " [[0.68605894]\n",
      " [1.7212286 ]\n",
      " [1.129252  ]\n",
      " [0.38489002]\n",
      " [0.81075245]\n",
      " [0.63276154]\n",
      " [0.35785586]\n",
      " [0.25469643]]\n",
      "7 Cost:  0.1704599 \n",
      "Prediction:\n",
      " [[0.6860479 ]\n",
      " [1.7212156 ]\n",
      " [1.1292417 ]\n",
      " [0.38488245]\n",
      " [0.81074333]\n",
      " [0.6327525 ]\n",
      " [0.3578496 ]\n",
      " [0.2546895 ]]\n",
      "8 Cost:  0.17045486 \n",
      "Prediction:\n",
      " [[0.6860369 ]\n",
      " [1.7212026 ]\n",
      " [1.1292312 ]\n",
      " [0.38487488]\n",
      " [0.81073433]\n",
      " [0.63274366]\n",
      " [0.35784322]\n",
      " [0.2546826 ]]\n",
      "9 Cost:  0.17044985 \n",
      "Prediction:\n",
      " [[0.68602574]\n",
      " [1.7211897 ]\n",
      " [1.1292207 ]\n",
      " [0.38486737]\n",
      " [0.8107251 ]\n",
      " [0.6327347 ]\n",
      " [0.3578369 ]\n",
      " [0.25467563]]\n",
      "10 Cost:  0.1704448 \n",
      "Prediction:\n",
      " [[0.6860146 ]\n",
      " [1.7211766 ]\n",
      " [1.1292102 ]\n",
      " [0.38485974]\n",
      " [0.810716  ]\n",
      " [0.6327258 ]\n",
      " [0.35783058]\n",
      " [0.25466865]]\n",
      "11 Cost:  0.17043975 \n",
      "Prediction:\n",
      " [[0.68600357]\n",
      " [1.7211636 ]\n",
      " [1.1291999 ]\n",
      " [0.38485217]\n",
      " [0.81070673]\n",
      " [0.63271683]\n",
      " [0.3578242 ]\n",
      " [0.25466174]]\n",
      "12 Cost:  0.17043474 \n",
      "Prediction:\n",
      " [[0.68599254]\n",
      " [1.7211506 ]\n",
      " [1.1291895 ]\n",
      " [0.38484454]\n",
      " [0.8106976 ]\n",
      " [0.6327079 ]\n",
      " [0.35781795]\n",
      " [0.25465482]]\n",
      "13 Cost:  0.1704298 \n",
      "Prediction:\n",
      " [[0.6859814 ]\n",
      " [1.721138  ]\n",
      " [1.129179  ]\n",
      " [0.38483703]\n",
      " [0.8106885 ]\n",
      " [0.63269895]\n",
      " [0.35781157]\n",
      " [0.25464785]]\n",
      "14 Cost:  0.1704248 \n",
      "Prediction:\n",
      " [[0.68597025]\n",
      " [1.7211251 ]\n",
      " [1.1291687 ]\n",
      " [0.38482946]\n",
      " [0.81067926]\n",
      " [0.63269   ]\n",
      " [0.35780525]\n",
      " [0.25464094]]\n",
      "15 Cost:  0.17041971 \n",
      "Prediction:\n",
      " [[0.6859592 ]\n",
      " [1.7211119 ]\n",
      " [1.1291581 ]\n",
      " [0.38482183]\n",
      " [0.81067014]\n",
      " [0.6326811 ]\n",
      " [0.35779893]\n",
      " [0.25463396]]\n",
      "16 Cost:  0.17041463 \n",
      "Prediction:\n",
      " [[0.6859482 ]\n",
      " [1.7210987 ]\n",
      " [1.1291478 ]\n",
      " [0.38481426]\n",
      " [0.8106609 ]\n",
      " [0.63267213]\n",
      " [0.35779262]\n",
      " [0.25462705]]\n",
      "17 Cost:  0.17040962 \n",
      "Prediction:\n",
      " [[0.68593705]\n",
      " [1.7210858 ]\n",
      " [1.1291373 ]\n",
      " [0.3848067 ]\n",
      " [0.8106518 ]\n",
      " [0.6326632 ]\n",
      " [0.3577863 ]\n",
      " [0.25462008]]\n",
      "18 Cost:  0.17040463 \n",
      "Prediction:\n",
      " [[0.6859259 ]\n",
      " [1.7210729 ]\n",
      " [1.129127  ]\n",
      " [0.38479918]\n",
      " [0.81064254]\n",
      " [0.63265425]\n",
      " [0.35777992]\n",
      " [0.25461316]]\n",
      "19 Cost:  0.17039967 \n",
      "Prediction:\n",
      " [[0.6859149 ]\n",
      " [1.7210602 ]\n",
      " [1.1291164 ]\n",
      " [0.3847915 ]\n",
      " [0.81063354]\n",
      " [0.6326455 ]\n",
      " [0.35777366]\n",
      " [0.2546062 ]]\n",
      "20 Cost:  0.17039463 \n",
      "Prediction:\n",
      " [[0.68590385]\n",
      " [1.7210472 ]\n",
      " [1.129106  ]\n",
      " [0.38478404]\n",
      " [0.8106244 ]\n",
      " [0.63263637]\n",
      " [0.35776728]\n",
      " [0.25459927]]\n",
      "21 Cost:  0.17038958 \n",
      "Prediction:\n",
      " [[0.6858927 ]\n",
      " [1.721034  ]\n",
      " [1.1290957 ]\n",
      " [0.3847764 ]\n",
      " [0.81061506]\n",
      " [0.63262755]\n",
      " [0.35776097]\n",
      " [0.2545923 ]]\n",
      "22 Cost:  0.17038454 \n",
      "Prediction:\n",
      " [[0.68588156]\n",
      " [1.7210209 ]\n",
      " [1.1290853 ]\n",
      " [0.38476884]\n",
      " [0.81060594]\n",
      " [0.6326186 ]\n",
      " [0.35775465]\n",
      " [0.2545854 ]]\n",
      "23 Cost:  0.17037947 \n",
      "Prediction:\n",
      " [[0.6858705 ]\n",
      " [1.721008  ]\n",
      " [1.1290747 ]\n",
      " [0.3847612 ]\n",
      " [0.8105967 ]\n",
      " [0.63260967]\n",
      " [0.35774833]\n",
      " [0.2545784 ]]\n",
      "24 Cost:  0.17037451 \n",
      "Prediction:\n",
      " [[0.6858595 ]\n",
      " [1.7209952 ]\n",
      " [1.1290643 ]\n",
      " [0.38475364]\n",
      " [0.8105876 ]\n",
      " [0.6326007 ]\n",
      " [0.357742  ]\n",
      " [0.2545715 ]]\n",
      "25 Cost:  0.17036952 \n",
      "Prediction:\n",
      " [[0.68584836]\n",
      " [1.7209823 ]\n",
      " [1.129054  ]\n",
      " [0.38474607]\n",
      " [0.81057835]\n",
      " [0.63259184]\n",
      " [0.35773563]\n",
      " [0.25456452]]\n",
      "26 Cost:  0.17036453 \n",
      "Prediction:\n",
      " [[0.6858372 ]\n",
      " [1.7209694 ]\n",
      " [1.1290436 ]\n",
      " [0.3847385 ]\n",
      " [0.8105692 ]\n",
      " [0.63258284]\n",
      " [0.35772938]\n",
      " [0.25455755]]\n",
      "27 Cost:  0.17035946 \n",
      "Prediction:\n",
      " [[0.6858262 ]\n",
      " [1.7209562 ]\n",
      " [1.1290331 ]\n",
      " [0.38473088]\n",
      " [0.8105602 ]\n",
      " [0.63257396]\n",
      " [0.357723  ]\n",
      " [0.25455058]]\n",
      "28 Cost:  0.17035435 \n",
      "Prediction:\n",
      " [[0.68581516]\n",
      " [1.720943  ]\n",
      " [1.1290226 ]\n",
      " [0.38472337]\n",
      " [0.810551  ]\n",
      " [0.63256496]\n",
      " [0.35771668]\n",
      " [0.25454366]]\n",
      "29 Cost:  0.17034936 \n",
      "Prediction:\n",
      " [[0.685804  ]\n",
      " [1.7209301 ]\n",
      " [1.1290122 ]\n",
      " [0.3847158 ]\n",
      " [0.81054175]\n",
      " [0.632556  ]\n",
      " [0.35771036]\n",
      " [0.2545367 ]]\n",
      "30 Cost:  0.17034444 \n",
      "Prediction:\n",
      " [[0.68579286]\n",
      " [1.7209175 ]\n",
      " [1.1290019 ]\n",
      " [0.38470823]\n",
      " [0.81053275]\n",
      " [0.63254726]\n",
      " [0.35770404]\n",
      " [0.25452977]]\n",
      "31 Cost:  0.1703394 \n",
      "Prediction:\n",
      " [[0.68578184]\n",
      " [1.7209045 ]\n",
      " [1.1289915 ]\n",
      " [0.3847006 ]\n",
      " [0.8105234 ]\n",
      " [0.6325382 ]\n",
      " [0.35769773]\n",
      " [0.2545228 ]]\n",
      "32 Cost:  0.17033435 \n",
      "Prediction:\n",
      " [[0.6857708 ]\n",
      " [1.7208915 ]\n",
      " [1.1289809 ]\n",
      " [0.3846931 ]\n",
      " [0.8105143 ]\n",
      " [0.6325293 ]\n",
      " [0.35769135]\n",
      " [0.2545159 ]]\n",
      "33 Cost:  0.17032933 \n",
      "Prediction:\n",
      " [[0.68575966]\n",
      " [1.7208784 ]\n",
      " [1.1289705 ]\n",
      " [0.38468546]\n",
      " [0.81050515]\n",
      " [0.63252044]\n",
      " [0.35768503]\n",
      " [0.25450897]]\n",
      "34 Cost:  0.17032434 \n",
      "Prediction:\n",
      " [[0.6857485 ]\n",
      " [1.7208655 ]\n",
      " [1.1289601 ]\n",
      " [0.3846779 ]\n",
      " [0.81049603]\n",
      " [0.63251144]\n",
      " [0.3576787 ]\n",
      " [0.254502  ]]\n",
      "35 Cost:  0.17031932 \n",
      "Prediction:\n",
      " [[0.6857375 ]\n",
      " [1.7208525 ]\n",
      " [1.1289496 ]\n",
      " [0.38467032]\n",
      " [0.8104869 ]\n",
      " [0.63250256]\n",
      " [0.3576724 ]\n",
      " [0.25449502]]\n",
      "36 Cost:  0.17031425 \n",
      "Prediction:\n",
      " [[0.68572646]\n",
      " [1.7208395 ]\n",
      " [1.1289392 ]\n",
      " [0.38466275]\n",
      " [0.81047755]\n",
      " [0.6324936 ]\n",
      " [0.35766608]\n",
      " [0.2544881 ]]\n",
      "37 Cost:  0.17030929 \n",
      "Prediction:\n",
      " [[0.6857153 ]\n",
      " [1.7208266 ]\n",
      " [1.1289289 ]\n",
      " [0.38465524]\n",
      " [0.81046844]\n",
      " [0.6324847 ]\n",
      " [0.3576597 ]\n",
      " [0.2544812 ]]\n",
      "38 Cost:  0.1703043 \n",
      "Prediction:\n",
      " [[0.6857042 ]\n",
      " [1.7208138 ]\n",
      " [1.1289184 ]\n",
      " [0.3846476 ]\n",
      " [0.81045943]\n",
      " [0.63247573]\n",
      " [0.35765344]\n",
      " [0.25447422]]\n",
      "39 Cost:  0.17029928 \n",
      "Prediction:\n",
      " [[0.68569314]\n",
      " [1.7208008 ]\n",
      " [1.128908  ]\n",
      " [0.3846401 ]\n",
      " [0.8104502 ]\n",
      " [0.6324668 ]\n",
      " [0.35764706]\n",
      " [0.2544673 ]]\n",
      "40 Cost:  0.17029423 \n",
      "Prediction:\n",
      " [[0.6856821 ]\n",
      " [1.7207878 ]\n",
      " [1.1288974 ]\n",
      " [0.3846324 ]\n",
      " [0.81044096]\n",
      " [0.6324578 ]\n",
      " [0.35764074]\n",
      " [0.25446033]]\n",
      "41 Cost:  0.17028922 \n",
      "Prediction:\n",
      " [[0.685671  ]\n",
      " [1.7207747 ]\n",
      " [1.1288872 ]\n",
      " [0.38462484]\n",
      " [0.81043196]\n",
      " [0.6324489 ]\n",
      " [0.35763443]\n",
      " [0.25445342]]\n",
      "42 Cost:  0.17028418 \n",
      "Prediction:\n",
      " [[0.6856598 ]\n",
      " [1.7207618 ]\n",
      " [1.1288767 ]\n",
      " [0.38461727]\n",
      " [0.8104226 ]\n",
      " [0.63244   ]\n",
      " [0.35762805]\n",
      " [0.25444645]]\n",
      "43 Cost:  0.17027918 \n",
      "Prediction:\n",
      " [[0.6856488 ]\n",
      " [1.7207488 ]\n",
      " [1.1288663 ]\n",
      " [0.3846097 ]\n",
      " [0.8104135 ]\n",
      " [0.63243115]\n",
      " [0.3576218 ]\n",
      " [0.25443953]]\n",
      "44 Cost:  0.17027423 \n",
      "Prediction:\n",
      " [[0.6856378 ]\n",
      " [1.720736  ]\n",
      " [1.128856  ]\n",
      " [0.38460213]\n",
      " [0.81040436]\n",
      " [0.6324222 ]\n",
      " [0.3576154 ]\n",
      " [0.25443256]]\n",
      "45 Cost:  0.17026912 \n",
      "Prediction:\n",
      " [[0.6856266 ]\n",
      " [1.7207227 ]\n",
      " [1.1288455 ]\n",
      " [0.38459456]\n",
      " [0.81039524]\n",
      " [0.63241327]\n",
      " [0.3576091 ]\n",
      " [0.25442564]]\n",
      "46 Cost:  0.17026417 \n",
      "Prediction:\n",
      " [[0.6856155 ]\n",
      " [1.72071   ]\n",
      " [1.128835  ]\n",
      " [0.38458693]\n",
      " [0.8103861 ]\n",
      " [0.6324043 ]\n",
      " [0.35760278]\n",
      " [0.25441867]]\n",
      "47 Cost:  0.1702591 \n",
      "Prediction:\n",
      " [[0.68560445]\n",
      " [1.7206968 ]\n",
      " [1.1288246 ]\n",
      " [0.38457942]\n",
      " [0.810377  ]\n",
      " [0.6323954 ]\n",
      " [0.3575964 ]\n",
      " [0.25441176]]\n",
      "48 Cost:  0.17025407 \n",
      "Prediction:\n",
      " [[0.6855934 ]\n",
      " [1.7206838 ]\n",
      " [1.1288142 ]\n",
      " [0.38457185]\n",
      " [0.81036764]\n",
      " [0.6323864 ]\n",
      " [0.35759014]\n",
      " [0.25440478]]\n",
      "49 Cost:  0.1702491 \n",
      "Prediction:\n",
      " [[0.6855823 ]\n",
      " [1.7206709 ]\n",
      " [1.128804  ]\n",
      " [0.38456428]\n",
      " [0.8103585 ]\n",
      " [0.6323775 ]\n",
      " [0.35758376]\n",
      " [0.25439787]]\n",
      "50 Cost:  0.1702441 \n",
      "Prediction:\n",
      " [[0.68557113]\n",
      " [1.7206581 ]\n",
      " [1.1287935 ]\n",
      " [0.38455665]\n",
      " [0.8103493 ]\n",
      " [0.63236856]\n",
      " [0.35757744]\n",
      " [0.2543909 ]]\n",
      "51 Cost:  0.17023906 \n",
      "Prediction:\n",
      " [[0.6855601 ]\n",
      " [1.7206451 ]\n",
      " [1.1287829 ]\n",
      " [0.38454908]\n",
      " [0.8103403 ]\n",
      " [0.6323596 ]\n",
      " [0.35757113]\n",
      " [0.25438398]]\n",
      "52 Cost:  0.17023405 \n",
      "Prediction:\n",
      " [[0.6855491 ]\n",
      " [1.7206321 ]\n",
      " [1.1287725 ]\n",
      " [0.3845415 ]\n",
      " [0.81033117]\n",
      " [0.6323507 ]\n",
      " [0.3575648 ]\n",
      " [0.254377  ]]\n",
      "53 Cost:  0.17022899 \n",
      "Prediction:\n",
      " [[0.68553793]\n",
      " [1.720619  ]\n",
      " [1.128762  ]\n",
      " [0.38453394]\n",
      " [0.8103218 ]\n",
      " [0.63234174]\n",
      " [0.3575585 ]\n",
      " [0.2543701 ]]\n",
      "54 Cost:  0.17022406 \n",
      "Prediction:\n",
      " [[0.6855268 ]\n",
      " [1.7206063 ]\n",
      " [1.1287515 ]\n",
      " [0.38452637]\n",
      " [0.8103128 ]\n",
      " [0.632333  ]\n",
      " [0.3575521 ]\n",
      " [0.25436312]]\n",
      "55 Cost:  0.17021902 \n",
      "Prediction:\n",
      " [[0.68551576]\n",
      " [1.7205933 ]\n",
      " [1.1287411 ]\n",
      " [0.3845188 ]\n",
      " [0.81030357]\n",
      " [0.63232386]\n",
      " [0.3575458 ]\n",
      " [0.2543562 ]]\n",
      "56 Cost:  0.17021401 \n",
      "Prediction:\n",
      " [[0.68550473]\n",
      " [1.7205803 ]\n",
      " [1.1287308 ]\n",
      " [0.38451123]\n",
      " [0.81029445]\n",
      " [0.63231504]\n",
      " [0.35753947]\n",
      " [0.2543493 ]]\n",
      "57 Cost:  0.17020896 \n",
      "Prediction:\n",
      " [[0.6854936 ]\n",
      " [1.7205672 ]\n",
      " [1.1287204 ]\n",
      " [0.3845036 ]\n",
      " [0.8102851 ]\n",
      " [0.6323061 ]\n",
      " [0.35753316]\n",
      " [0.25434232]]\n",
      "58 Cost:  0.17020395 \n",
      "Prediction:\n",
      " [[0.68548244]\n",
      " [1.7205544 ]\n",
      " [1.1287098 ]\n",
      " [0.38449615]\n",
      " [0.810276  ]\n",
      " [0.63229716]\n",
      " [0.35752684]\n",
      " [0.25433534]]\n",
      "59 Cost:  0.17019892 \n",
      "Prediction:\n",
      " [[0.6854714 ]\n",
      " [1.7205411 ]\n",
      " [1.1286994 ]\n",
      " [0.38448846]\n",
      " [0.810267  ]\n",
      " [0.6322883 ]\n",
      " [0.35752052]\n",
      " [0.25432843]]\n",
      "60 Cost:  0.17019388 \n",
      "Prediction:\n",
      " [[0.6854604 ]\n",
      " [1.7205281 ]\n",
      " [1.128689  ]\n",
      " [0.38448095]\n",
      " [0.81025773]\n",
      " [0.63227934]\n",
      " [0.3575142 ]\n",
      " [0.25432152]]\n",
      "61 Cost:  0.17018887 \n",
      "Prediction:\n",
      " [[0.68544924]\n",
      " [1.7205153 ]\n",
      " [1.1286786 ]\n",
      " [0.38447332]\n",
      " [0.8102486 ]\n",
      " [0.63227034]\n",
      " [0.35750782]\n",
      " [0.25431454]]\n",
      "62 Cost:  0.17018397 \n",
      "Prediction:\n",
      " [[0.6854381 ]\n",
      " [1.7205026 ]\n",
      " [1.1286683 ]\n",
      " [0.38446575]\n",
      " [0.8102395 ]\n",
      " [0.63226146]\n",
      " [0.3575015 ]\n",
      " [0.25430763]]\n",
      "63 Cost:  0.1701789 \n",
      "Prediction:\n",
      " [[0.68542707]\n",
      " [1.7204894 ]\n",
      " [1.1286579 ]\n",
      " [0.38445818]\n",
      " [0.8102304 ]\n",
      " [0.63225245]\n",
      " [0.3574952 ]\n",
      " [0.25430065]]\n",
      "64 Cost:  0.17017387 \n",
      "Prediction:\n",
      " [[0.68541604]\n",
      " [1.7204764 ]\n",
      " [1.1286473 ]\n",
      " [0.3844506 ]\n",
      " [0.81022125]\n",
      " [0.6322435 ]\n",
      " [0.35748887]\n",
      " [0.25429374]]\n",
      "65 Cost:  0.17016886 \n",
      "Prediction:\n",
      " [[0.6854049 ]\n",
      " [1.7204635 ]\n",
      " [1.1286368 ]\n",
      " [0.38444304]\n",
      " [0.8102119 ]\n",
      " [0.63223463]\n",
      " [0.35748255]\n",
      " [0.25428677]]\n",
      "66 Cost:  0.1701639 \n",
      "Prediction:\n",
      " [[0.68539375]\n",
      " [1.7204506 ]\n",
      " [1.1286266 ]\n",
      " [0.38443547]\n",
      " [0.8102028 ]\n",
      " [0.6322257 ]\n",
      " [0.35747617]\n",
      " [0.25427985]]\n",
      "67 Cost:  0.17015888 \n",
      "Prediction:\n",
      " [[0.6853827 ]\n",
      " [1.7204376 ]\n",
      " [1.1286161 ]\n",
      " [0.38442785]\n",
      " [0.81019366]\n",
      " [0.6322169 ]\n",
      " [0.35746992]\n",
      " [0.25427288]]\n",
      "68 Cost:  0.17015386 \n",
      "Prediction:\n",
      " [[0.6853717 ]\n",
      " [1.7204247 ]\n",
      " [1.1286056 ]\n",
      " [0.38442028]\n",
      " [0.81018454]\n",
      " [0.63220793]\n",
      " [0.35746354]\n",
      " [0.25426596]]\n",
      "69 Cost:  0.17014885 \n",
      "Prediction:\n",
      " [[0.68536055]\n",
      " [1.7204118 ]\n",
      " [1.1285952 ]\n",
      " [0.38441265]\n",
      " [0.8101752 ]\n",
      " [0.6321989 ]\n",
      " [0.35745722]\n",
      " [0.254259  ]]\n",
      "70 Cost:  0.17014387 \n",
      "Prediction:\n",
      " [[0.6853494 ]\n",
      " [1.7203989 ]\n",
      " [1.1285849 ]\n",
      " [0.38440508]\n",
      " [0.8101662 ]\n",
      " [0.63219005]\n",
      " [0.3574509 ]\n",
      " [0.25425208]]\n",
      "71 Cost:  0.17013875 \n",
      "Prediction:\n",
      " [[0.6853384 ]\n",
      " [1.7203854 ]\n",
      " [1.1285744 ]\n",
      " [0.38439757]\n",
      " [0.81015694]\n",
      " [0.6321811 ]\n",
      " [0.35744458]\n",
      " [0.2542451 ]]\n",
      "72 Cost:  0.17013378 \n",
      "Prediction:\n",
      " [[0.68532735]\n",
      " [1.7203727 ]\n",
      " [1.1285641 ]\n",
      " [0.38439   ]\n",
      " [0.8101478 ]\n",
      " [0.6321721 ]\n",
      " [0.35743827]\n",
      " [0.25423813]]\n",
      "73 Cost:  0.17012879 \n",
      "Prediction:\n",
      " [[0.6853162 ]\n",
      " [1.7203598 ]\n",
      " [1.1285536 ]\n",
      " [0.38438237]\n",
      " [0.8101387 ]\n",
      " [0.6321632 ]\n",
      " [0.3574319 ]\n",
      " [0.25423115]]\n",
      "74 Cost:  0.17012382 \n",
      "Prediction:\n",
      " [[0.68530506]\n",
      " [1.7203469 ]\n",
      " [1.1285434 ]\n",
      " [0.38437486]\n",
      " [0.81012934]\n",
      " [0.6321543 ]\n",
      " [0.35742563]\n",
      " [0.25422424]]\n",
      "75 Cost:  0.17011878 \n",
      "Prediction:\n",
      " [[0.68529403]\n",
      " [1.7203339 ]\n",
      " [1.1285328 ]\n",
      " [0.38436723]\n",
      " [0.8101202 ]\n",
      " [0.63214535]\n",
      " [0.35741925]\n",
      " [0.25421727]]\n",
      "76 Cost:  0.17011368 \n",
      "Prediction:\n",
      " [[0.685283  ]\n",
      " [1.7203207 ]\n",
      " [1.1285222 ]\n",
      " [0.38435966]\n",
      " [0.8101111 ]\n",
      " [0.6321364 ]\n",
      " [0.35741293]\n",
      " [0.25421035]]\n",
      "77 Cost:  0.1701087 \n",
      "Prediction:\n",
      " [[0.68527186]\n",
      " [1.7203078 ]\n",
      " [1.1285118 ]\n",
      " [0.3843522 ]\n",
      " [0.810102  ]\n",
      " [0.6321274 ]\n",
      " [0.35740662]\n",
      " [0.25420338]]\n",
      "78 Cost:  0.17010373 \n",
      "Prediction:\n",
      " [[0.6852607 ]\n",
      " [1.720295  ]\n",
      " [1.1285014 ]\n",
      " [0.38434458]\n",
      " [0.81009287]\n",
      " [0.63211864]\n",
      " [0.35740024]\n",
      " [0.25419647]]\n",
      "79 Cost:  0.17009872 \n",
      "Prediction:\n",
      " [[0.6852497 ]\n",
      " [1.720282  ]\n",
      " [1.128491  ]\n",
      " [0.384337  ]\n",
      " [0.81008375]\n",
      " [0.6321096 ]\n",
      " [0.35739398]\n",
      " [0.2541895 ]]\n",
      "80 Cost:  0.17009369 \n",
      "Prediction:\n",
      " [[0.68523866]\n",
      " [1.720269  ]\n",
      " [1.1284807 ]\n",
      " [0.38432938]\n",
      " [0.8100744 ]\n",
      " [0.63210076]\n",
      " [0.3573876 ]\n",
      " [0.25418258]]\n",
      "81 Cost:  0.17008874 \n",
      "Prediction:\n",
      " [[0.6852275 ]\n",
      " [1.7202561 ]\n",
      " [1.1284702 ]\n",
      " [0.3843218 ]\n",
      " [0.8100654 ]\n",
      " [0.6320918 ]\n",
      " [0.35738134]\n",
      " [0.25417566]]\n",
      "82 Cost:  0.17008373 \n",
      "Prediction:\n",
      " [[0.68521637]\n",
      " [1.7202432 ]\n",
      " [1.1284597 ]\n",
      " [0.38431424]\n",
      " [0.81005627]\n",
      " [0.6320829 ]\n",
      " [0.35737497]\n",
      " [0.2541687 ]]\n",
      "83 Cost:  0.17007865 \n",
      "Prediction:\n",
      " [[0.68520534]\n",
      " [1.72023   ]\n",
      " [1.1284493 ]\n",
      " [0.38430667]\n",
      " [0.81004703]\n",
      " [0.632074  ]\n",
      " [0.3573686 ]\n",
      " [0.25416172]]\n",
      "84 Cost:  0.17007363 \n",
      "Prediction:\n",
      " [[0.6851943 ]\n",
      " [1.720217  ]\n",
      " [1.1284387 ]\n",
      " [0.38429904]\n",
      " [0.8100379 ]\n",
      " [0.632065  ]\n",
      " [0.35736233]\n",
      " [0.2541548 ]]\n",
      "85 Cost:  0.17006865 \n",
      "Prediction:\n",
      " [[0.68518317]\n",
      " [1.7202041 ]\n",
      " [1.1284286 ]\n",
      " [0.38429153]\n",
      " [0.81002855]\n",
      " [0.63205606]\n",
      " [0.35735595]\n",
      " [0.2541479 ]]\n",
      "86 Cost:  0.17006375 \n",
      "Prediction:\n",
      " [[0.685172  ]\n",
      " [1.7201915 ]\n",
      " [1.1284182 ]\n",
      " [0.3842839 ]\n",
      " [0.81001955]\n",
      " [0.6320471 ]\n",
      " [0.3573497 ]\n",
      " [0.2541409 ]]\n",
      "87 Cost:  0.1700587 \n",
      "Prediction:\n",
      " [[0.685161  ]\n",
      " [1.7201785 ]\n",
      " [1.1284076 ]\n",
      " [0.3842764 ]\n",
      " [0.8100103 ]\n",
      " [0.6320382 ]\n",
      " [0.35734332]\n",
      " [0.254134  ]]\n",
      "88 Cost:  0.17005363 \n",
      "Prediction:\n",
      " [[0.68514997]\n",
      " [1.7201653 ]\n",
      " [1.1283972 ]\n",
      " [0.3842687 ]\n",
      " [0.8100012 ]\n",
      " [0.63202924]\n",
      " [0.35733694]\n",
      " [0.25412703]]\n",
      "89 Cost:  0.1700486 \n",
      "Prediction:\n",
      " [[0.6851388 ]\n",
      " [1.7201521 ]\n",
      " [1.1283867 ]\n",
      " [0.3842612 ]\n",
      " [0.8099921 ]\n",
      " [0.6320205 ]\n",
      " [0.35733068]\n",
      " [0.2541201 ]]\n",
      "90 Cost:  0.17004362 \n",
      "Prediction:\n",
      " [[0.6851277 ]\n",
      " [1.7201393 ]\n",
      " [1.1283762 ]\n",
      " [0.38425362]\n",
      " [0.80998296]\n",
      " [0.63201135]\n",
      " [0.3573243 ]\n",
      " [0.25411314]]\n",
      "91 Cost:  0.1700386 \n",
      "Prediction:\n",
      " [[0.68511665]\n",
      " [1.7201263 ]\n",
      " [1.1283659 ]\n",
      " [0.38424605]\n",
      " [0.8099737 ]\n",
      " [0.6320026 ]\n",
      " [0.35731798]\n",
      " [0.25410622]]\n",
      "92 Cost:  0.17003357 \n",
      "Prediction:\n",
      " [[0.6851056 ]\n",
      " [1.7201133 ]\n",
      " [1.1283555 ]\n",
      " [0.38423842]\n",
      " [0.8099645 ]\n",
      " [0.6319936 ]\n",
      " [0.35731167]\n",
      " [0.25409925]]\n",
      "93 Cost:  0.17002864 \n",
      "Prediction:\n",
      " [[0.6850945 ]\n",
      " [1.7201006 ]\n",
      " [1.128345  ]\n",
      " [0.3842309 ]\n",
      " [0.80995536]\n",
      " [0.63198465]\n",
      " [0.35730535]\n",
      " [0.25409234]]\n",
      "94 Cost:  0.1700236 \n",
      "Prediction:\n",
      " [[0.6850833 ]\n",
      " [1.7200875 ]\n",
      " [1.1283345 ]\n",
      " [0.38422328]\n",
      " [0.80994624]\n",
      " [0.63197577]\n",
      " [0.35729903]\n",
      " [0.25408536]]\n",
      "95 Cost:  0.17001857 \n",
      "Prediction:\n",
      " [[0.6850723 ]\n",
      " [1.7200743 ]\n",
      " [1.1283244 ]\n",
      " [0.3842157 ]\n",
      " [0.8099371 ]\n",
      " [0.6319668 ]\n",
      " [0.35729265]\n",
      " [0.25407845]]\n",
      "96 Cost:  0.17001359 \n",
      "Prediction:\n",
      " [[0.6850613 ]\n",
      " [1.7200615 ]\n",
      " [1.1283138 ]\n",
      " [0.38420808]\n",
      " [0.809928  ]\n",
      " [0.6319578 ]\n",
      " [0.3572864 ]\n",
      " [0.25407147]]\n",
      "97 Cost:  0.17000858 \n",
      "Prediction:\n",
      " [[0.68505013]\n",
      " [1.7200487 ]\n",
      " [1.1283033 ]\n",
      " [0.3842005 ]\n",
      " [0.80991876]\n",
      " [0.63194895]\n",
      " [0.35728002]\n",
      " [0.25406456]]\n",
      "98 Cost:  0.17000362 \n",
      "Prediction:\n",
      " [[0.685039  ]\n",
      " [1.7200358 ]\n",
      " [1.128293  ]\n",
      " [0.384193  ]\n",
      " [0.8099095 ]\n",
      " [0.63193995]\n",
      " [0.3572737 ]\n",
      " [0.2540576 ]]\n",
      "99 Cost:  0.1699986 \n",
      "Prediction:\n",
      " [[0.68502796]\n",
      " [1.7200228 ]\n",
      " [1.1282825 ]\n",
      " [0.38418543]\n",
      " [0.8099004 ]\n",
      " [0.63193107]\n",
      " [0.35726738]\n",
      " [0.25405067]]\n",
      "100 Cost:  0.16999352 \n",
      "Prediction:\n",
      " [[0.68501693]\n",
      " [1.7200096 ]\n",
      " [1.128272  ]\n",
      " [0.38417786]\n",
      " [0.8098913 ]\n",
      " [0.6319221 ]\n",
      " [0.35726106]\n",
      " [0.2540437 ]]\n"
     ]
    }
   ],
   "source": [
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, cost_val, hy_val = sess.run(\n",
    "            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.737067965\n",
      "Epoch: 0002, Cost: 1.117587513\n",
      "Epoch: 0003, Cost: 0.888121966\n",
      "Epoch: 0004, Cost: 0.775906514\n",
      "Epoch: 0005, Cost: 0.705798444\n",
      "Epoch: 0006, Cost: 0.655291358\n",
      "Epoch: 0007, Cost: 0.617271047\n",
      "Epoch: 0008, Cost: 0.587195333\n",
      "Epoch: 0009, Cost: 0.562602887\n",
      "Epoch: 0010, Cost: 0.541527019\n",
      "Epoch: 0011, Cost: 0.523908252\n",
      "Epoch: 0012, Cost: 0.508454488\n",
      "Epoch: 0013, Cost: 0.495084639\n",
      "Epoch: 0014, Cost: 0.483201224\n",
      "Epoch: 0015, Cost: 0.472191160\n",
      "Learning finished\n",
      "Accuracy:  0.8889\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.reduce_mean([1, 2], axis=0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
